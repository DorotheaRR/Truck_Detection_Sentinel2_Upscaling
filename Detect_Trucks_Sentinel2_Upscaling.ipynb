{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Trucks Sentinel-2 - Europe\n",
    "________________                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load creds\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 1 | Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# installations\n",
    "def install_package(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "install_package(\"OSMPythonTools\")\n",
    "install_package(\"geocube\")\n",
    "\n",
    "# OSM API\n",
    "from OSMPythonTools.overpass import overpassQueryBuilder, Overpass\n",
    "\n",
    "# xcube\n",
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "# spatial\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geocube\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "from shapely import geometry, coords\n",
    "from shapely.geometry import Polygon, Point\n",
    "#from osgeo import gdal #, gdal_array, ogr\n",
    "\n",
    "# plotting\n",
    "import matplotlib as plt\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2 | General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_results = True\n",
    "country = \"Germany\"\n",
    "max_observations = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # approx. 10m\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"SCL\"]\n",
    "day_bins = \"1D\" # for cube\n",
    "tile_size = [512, 512]\n",
    "minimum_valid_observations = 15 # percent\n",
    "grid_spacing = 1. # processing grid box size [degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main = os.getcwd()\n",
    "dir_not_commit = os.path.join(dir_main, \"not_commit\")\n",
    "dir_ancil = os.path.join(dir_not_commit, \"ancillary_data\")\n",
    "dirs = {\"dir\":dir_main, \"dir_not_commit\":dir_not_commit, \"ancil\":dir_ancil, \"processing\":os.path.join(dir_main, \"processing\"),\n",
    "       \"processed\":os.path.join(dir_not_commit, \"processed\"), \"ancil_roads\":os.path.join(dir_ancil, \"roads\"), \"ancil_gadm\":os.path.join(dir_ancil, \"gadm\")}\n",
    "for directory in list(dirs.values()):\n",
    "    if not os.path.exists(directory): os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\"gadm_efta\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_EU_EFTA.gpkg\"), \n",
    "         \"gadm_europe\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_europe.gpkg\"), \n",
    "         \"gadm_europe_union\":os.path.join(dirs[\"ancil_gadm\"], \"gadm0_europe_union.gpkg\"),\n",
    "         \"proc_grid\":os.path.join(dirs[\"processing\"], \"processing_grid_gadm_%s.geojson\" %(grid_spacing))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = [\"tuesday\", \"wednesday\", \"thursday\"] \n",
    "target = {\"first\":datetime(2020, 3, 16), \"last\":datetime(2020, 6, 16)}\n",
    "baseline = {\"first\":datetime(2019, 3, 16), \"last\":datetime(2020, 3, target[\"first\"].day - 1)}\n",
    "n_days_sub = 91 # days per sub-period\n",
    "timestamps_sub_period = 1 # how many aggregated timestamps per sub-period\n",
    "baseline_years = [2017, 2018, 2019] # if processing for a yearly period as in target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_key = \"highway\"\n",
    "osm_values = [\"motorway\", \"trunk\", \"primary\"]\n",
    "roads_buffer = 0.00022 # degree, for motorway, the others lower (see OSM methods below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_masking_thresholds = {\"rgb\":0.25,\n",
    "                            \"blue_green\":0.2,\n",
    "                            \"blue_red\":0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 3 | Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\"min_rgb\":0.05,\n",
    "              \"min_blue\":0.06,\n",
    "              \"max_red\":0.15,\n",
    "              \"max_green\":0.15,\n",
    "              \"max_blue\":0.2,\n",
    "              \"max_ndvi\":0.5,\n",
    "              \"max_ndwi\":0.0001,\n",
    "              \"max_ndsi\":0.0001,\n",
    "              \"min_blue_green_ratio\":0.03,\n",
    "              \"min_blue_red_ratio\":0.03, \n",
    "              \"max_blue_green_ratio\":0.18, \n",
    "              \"max_blue_red_ratio\":0.18}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## 4 | Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPSG_4326(): return \"EPSG:4326\"\n",
    "def EPSG_3857(): return \"EPSG:3857\"\n",
    "def GEOJSON(): return \"GeoJSON\"\n",
    "def GPKG(): return \"GPKG\"\n",
    "def GEOJSON_EXT(): return \".geojson\"\n",
    "def GPKG_EXT(): return \".gpkg\"\n",
    "def NC_EXT(): return \".nc\"\n",
    "def BBOX_ID(): return \"bbox_id\"\n",
    "def ABS_N_TRUCKS(): return \"acquisitions_trucks\"\n",
    "def TRUCKS_VEC(): return \"trucks_points\"\n",
    "def TRUCKS_VEC_PHR(): return \"trucks_points_placeholder\"\n",
    "def N_OBS(): return \"n_observations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname_osm(directory, bbox_id, osm_key, ext = GPKG_EXT()): return os.path.join(directory, str(bbox_id) + \"_\" + osm_key + ext)\n",
    "def construct_fname(dirs_ts, dir_ts_key, bbox_id, ext):\n",
    "    return os.path.join(dirs_ts[dir_ts_key], dir_ts_key + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_acquisition_trucks(dirs_ts, bbox_id, date, ext = NC_EXT()): \n",
    "    return os.path.join(dirs_ts[ABS_N_TRUCKS()], str(date) + \"_\" + ABS_N_TRUCKS() + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_sum_obs(dirs_ts, bbox_id, date, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, N_OBS(), str(bbox_id) + \"_\" + str(date), ext)\n",
    "def fname_trucks_vec(dirs_ts, bbox_id, date, ext):\n",
    "    return construct_fname(dirs_ts, TRUCKS_VEC(), str(bbox_id) + \"_\" + str(date), ext)\n",
    "def fname_trucks_vec_placeholder(dirs_ts, bbox_id, date, ext = \".txt\"):\n",
    "    return construct_fname(dirs_ts, TRUCKS_VEC(), str(bbox_id) + \"_\" + str(date), ext)\n",
    "\n",
    "def sub_period_fnames(dirs_ts, bbox_id, date, ext_arr, ext_vec):\n",
    "    files = {N_OBS():fname_sum_obs(dirs_ts, bbox_id, date, ext_arr),\n",
    "             TRUCKS_VEC():fname_trucks_vec(dirs_ts, bbox_id, date, ext_vec),\n",
    "             TRUCKS_VEC_PHR():fname_trucks_vec_placeholder(dirs_ts, bbox_id, date)}\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_ts(dir_ts):\n",
    "    dir_ts_overall = os.path.join(dir_ts, \"overall\")\n",
    "    if not os.path.exists(dir_ts_overall): os.mkdir(dir_ts_overall)\n",
    "    dirs_ts = {ABS_N_TRUCKS():os.path.join(dir_ts, ABS_N_TRUCKS()),\n",
    "               N_OBS():os.path.join(dir_ts_overall, N_OBS()),\n",
    "               TRUCKS_VEC():os.path.join(dir_ts_overall, TRUCKS_VEC())}\n",
    "    for direc in dirs_ts.values():\n",
    "        if not os.path.exists(direc): os.mkdir(direc)\n",
    "    return dirs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_recurs(directory, validation):\n",
    "    if validation == \"YES_DELETE_THAT\":\n",
    "        shutil.rmtree(directory)\n",
    "    else:\n",
    "        raise Exception(\"Validation wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_msg(weekdays, n_boxes, periods, timestamps_sub_period, osm_values, baseline_years, minimum_valid_observations, overwrite_results):\n",
    "    print(\"%s\\n\\nStarting truck detection processing \\n%s\\n\\nWeekdays: \\\n",
    "%s\\nNumber of grid cells to process: %s\\nNumber of periods: %s\\nTimestamps sub-period: \\\n",
    "%s\\nOSM roads: %s\\nBaseline years: %s\\nMinimum valid observations to consider acquisition: \\\n",
    "%s %%\\nOverwrite results: %s\\n%s\\n\\n%s\" %(\"=\"*100, \".\"*50, weekdays, str(n_boxes), str(len(periods[\"first\"])), \n",
    "                                          str(timestamps_sub_period), str(osm_values), str(baseline_years), \n",
    "                                          str(minimum_valid_observations), str(overwrite_results), \".\"*50, \"=\"*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing grid\n",
    "Create a grid covering Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(grid_spacing, files):\n",
    "    crs = EPSG_4326()\n",
    "    gadm0_eu_efta = gpd.read_file(files[\"gadm_efta\"])\n",
    "    xmin,ymin,xmax,ymax = gadm0_eu_efta.total_bounds\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    cols = int((width) / grid_spacing)\n",
    "    rows = int((height) / grid_spacing)\n",
    "    box_width = width / cols\n",
    "    box_height = height / rows\n",
    "    boxes = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            col_right = col + 1\n",
    "            row_lower = row + 1\n",
    "            y_up = ymax-row*box_height\n",
    "            y_low = ymax-row_lower*box_height\n",
    "            x_left = xmin+col*box_width\n",
    "            x_right = xmin+col_right*box_width\n",
    "            boxes.append(Polygon([(x_left, y_up), \n",
    "                                  (x_right, y_up), \n",
    "                                  (x_right, y_low), \n",
    "                                  (x_left, y_low)]))\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":boxes})\n",
    "    grid.crs = crs\n",
    "    gadm0_europe = gpd.read_file(files[\"gadm_europe\"])\n",
    "    gadm0_europe.crs = crs\n",
    "    file_union = files[\"gadm_europe_union\"]\n",
    "    if os.path.exists(file_union):\n",
    "        gadm0_europe_clip_union = gpd.read_file(file_union)\n",
    "    else:\n",
    "        gadm0_europe[\"continent\"] = [\"EUROPE\"] * len(gadm0_europe)\n",
    "        gadm0_europe_clip = gpd.overlay(gadm0_europe, gpd.GeoDataFrame({\"a\":[1],\"geometry\":test}), how = \"intersection\")\n",
    "        gadm0_europe_clip_union = gadm0_europe_clip.dissolve(by = \"continent\")\n",
    "        gadm0_europe_clip_union.to_file(file_union, driver = GPKG())\n",
    "    # intersect with gadm for cutting boxes at coast line\n",
    "    grid_intersect = gpd.overlay(grid, gadm0_europe_clip_union, how = \"intersection\")\n",
    "    # get country attributes\n",
    "    grid_gadm = gpd.sjoin(grid_intersect, gadm0_europe, how = \"left\", op = \"intersects\")\n",
    "    grid_gadm[BBOX_ID()] = range(len(grid_gadm))\n",
    "    boxes = grid_gadm.geometry.apply(lambda geom : geom.bounds)\n",
    "    grid_gadm.geometry = [Polygon(geometry.box(b[0], b[1], b[2], b[3])) for b in boxes] # use the light bboxes\n",
    "    delete_cols = [\"GID_0_left\", \"NAME_0_left\", \"a\", \"continent\", \"index_right\"]\n",
    "    for column in delete_cols:\n",
    "        if column in grid_gadm.columns: \n",
    "            grid_gadm = grid_gadm.drop(column, 1)\n",
    "    grid_gadm = grid_gadm.rename(columns = {\"GID_0_right\":\"GID_0\", \"NAME_0_right\":\"NAME_0\"})\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"GID_0\"] != \"RUS\"] # do not include\n",
    "    grid_gadm.to_file(files[\"proc_grid\"], driver = GEOJSON())\n",
    "    return grid_gadm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lat_lon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(polygons, lat, lon, fill=np.nan):\n",
    "    transform = transform_lat_lon(lat, lon)\n",
    "    out_shape = (len(lat), len(lon))\n",
    "    raster = features.rasterize(polygons.geometry, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float)\n",
    "    return xr.DataArray(raster, coords={\"lat\":lat, \"lon\":lon}, dims=(\"lat\", \"lon\"))\n",
    "\n",
    "def vec_driver_from_ext(ext):\n",
    "    return {GPKG_EXT():GPKG(), GEOJSON_EXT():GEOJSON()}[ext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data np array\n",
    "# lat_lon dict of \"lat\" and \"lon\" holding np arrays of coordinates\n",
    "def create_xr_dataset(data, lat_lon, name):\n",
    "    lat, lon = \"lat\", \"lon\"\n",
    "    data_array = xr.DataArray(data, coords={lat:lat_lon[lat], lon:lat_lon[lon]}, dims=(lat, lon))\n",
    "    return xr.Dataset({name:data_array})\n",
    "\n",
    "# extracts coordinates at value in np array and returns points as GeoDataFrame\n",
    "# data 2d np array\n",
    "# match_value Float value in data where point coordinates are extracted\n",
    "# lon_lat dict of:\n",
    "### \"lon\": np array longitude values\"\n",
    "### \"lat\": np array latitude values\"\n",
    "# crs String EPSG:XXXX\n",
    "def points_from_np(data, match_value, lon_lat, crs):\n",
    "    indices = np.argwhere(data == match_value)\n",
    "    if len(indices) > 0:\n",
    "        lat_indices = indices[:,[0]]\n",
    "        lon_indices = indices[:,[1]]\n",
    "        lat_coords = lon_lat[\"lat\"][lat_indices]\n",
    "        lon_coords = lon_lat[\"lon\"][lon_indices]\n",
    "        points = gpd.GeoDataFrame(geometry = gpd.points_from_xy(lon_coords, lat_coords))\n",
    "        points.crs = crs\n",
    "        return points\n",
    "    \n",
    "def raster_to_points(raster, lon_lat, field_name, crs):\n",
    "    points_list = []\n",
    "    match_values = np.unique(raster[(raster != 0) * ~np.isnan(raster)]) # by pixel value\n",
    "    for x in match_values:\n",
    "        points = points_from_np(raster, x, lon_lat, crs=crs)\n",
    "        points[field_name] = [x] * len(points)\n",
    "        points_list.append(points)\n",
    "    return gpd.GeoDataFrame(pd.concat(points_list, ignore_index=True))\n",
    "\n",
    "# take xarray and ensure each value with 1 in data has no neighbor with 1 in an extended 3x3 block. Extended means: horizontally and vertically\n",
    "# it is also checked for the second-next pixel\n",
    "# Method checks only surrounding of values equal 1\n",
    "# arr xarray DataArray with values and lat lon\n",
    "def filter_spatial_3x3_extended(arr):\n",
    "    values = arr.values\n",
    "    lon = arr.lon\n",
    "    lat = arr.lat\n",
    "    valid = np.where(arr == 1)\n",
    "    for y,x in zip(valid[0], valid[1]):\n",
    "        y_above = y - 1\n",
    "        y_above_next = y - 2\n",
    "        x_left = x - 1\n",
    "        x_right = x + 1\n",
    "        x_left_next = x - 2\n",
    "        space_left = x_left >= 0\n",
    "        space_right = x_right >= 0 and x_right < len(lon)\n",
    "        space_above = y_above >= 0\n",
    "        val_left_above = values[y_above, x_left] if space_left and space_above else 0\n",
    "        val_right_above = values[y_above, x_right] if space_right and space_above else 0\n",
    "        val_left = values[y, x_left] if space_left else 0\n",
    "        val_above = values[y_above, x] if space_above else 0\n",
    "        val_left_next = values[y, x_left_next] if x_left_next >= 0 else 0\n",
    "        val_above_next = values[y_above_next, x] if y_above_next >= 0 else 0\n",
    "        # if any of the values left, above and left above has 1 set current value 0\n",
    "        if (val_left_above + val_right_above + val_left + val_above + val_left_next + val_above_next) >= 1:\n",
    "            values[y,x] = 0\n",
    "    arr.values = values\n",
    "    return arr\n",
    "\n",
    "def calc_rgb_cloud_mask(band_stack, cloud_masking_thresholds):\n",
    "    B02, B03, B04 = band_stack.B02, band_stack.B03, band_stack.B04\n",
    "    c = cloud_masking_thresholds[\"rgb\"]\n",
    "    clouds_rgb = ((B02 > c) + (B03 > c) + (B04 > c)) >= 1\n",
    "    # attempt to mask haze without masking out truck pixels (similar! higher blue than red and green)\n",
    "    blue_green_ratio = (B02-B03) / (B02+B03)\n",
    "    blue_red_ratio = (B02-B04) / (B02+B04)\n",
    "    clouds_blue_green = blue_green_ratio > cloud_masking_thresholds[\"blue_green\"]\n",
    "    clouds_blue_red = blue_red_ratio > cloud_masking_thresholds[\"blue_red\"]\n",
    "    clouds = (clouds_rgb + clouds_blue_green + clouds_blue_red) >= 1\n",
    "    return clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date Datetime object to be checked\n",
    "# weekday String weekday to be checked against\n",
    "# returns Boolean if date is weekday\n",
    "def is_weekday(date_x, weekday):\n",
    "    if not isinstance(date_x, type(datetime.date)): date_x = pd.to_datetime(date_x).date()\n",
    "    weekday = weekday.lower()[0:3]\n",
    "    y, m, d = 2000, 1, 3\n",
    "    wd = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "    ref = {}\n",
    "    for i in range(len(wd)): ref[wd[i]] = datetime(y, m, d + i).date()\n",
    "    return (date_x - ref[weekday]).days % 7 == 0 # check if date is in a sequence of 7\n",
    "\n",
    "# Calculates the dates of the periods representing a timestamp each\n",
    "# n_days_sub Integer how many dates shall one timestamp (sub-period) cover\n",
    "# baseline Dict start and end dates (datetime) overall baseline period\n",
    "# target Dict start and end dates (datetime) target period\n",
    "def calc_periods(n_days_sub, baseline, target):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    base_first = baseline[fst]\n",
    "    n_days = target[fst] - base_first\n",
    "    n_sub = int(n_days.days / n_days_sub) # number of timestamps\n",
    "    start = [base_first] * n_sub\n",
    "    start = [start[i] + timedelta(n_days_sub * i) for i in range(len(start))]\n",
    "    end = [start[i+1] - timedelta(1) for i in range(len(start)-1)]\n",
    "    end.append(baseline[lst])\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    periods[\"first\"].append(target[\"first\"]) # append start date of target period\n",
    "    periods[\"last\"].append(target[\"last\"]) # append end date of target period\n",
    "    return periods\n",
    "\n",
    "# Returns periods equivalent to target period for other years\n",
    "# target Dict start and end dates (datetime) target period\n",
    "# years List of int years\n",
    "# timestamps_sub_period int\n",
    "def yearly_period_from_target(target, years = [2017, 2018, 2019], timestamps_sub_period = 2):\n",
    "    n_days_target = (target[\"last\"]-target[\"first\"]).days\n",
    "    n_days_timestamp = int(n_days_target / timestamps_sub_period)\n",
    "    trgt_fst = []\n",
    "    trgt_lst = []\n",
    "    for i in range(timestamps_sub_period):\n",
    "        start = target[\"first\"] + timedelta(days = i * (n_days_timestamp + 1))\n",
    "        trgt_fst.append(start)\n",
    "        trgt_lst.append(start + timedelta(days = n_days_timestamp))\n",
    "    start = []\n",
    "    end = []\n",
    "    for year in years:\n",
    "        for fst, lst in zip(trgt_fst, trgt_lst):\n",
    "            start.append(datetime(year, fst.month, fst.day))\n",
    "            end.append(datetime(year, lst.month, lst.day))\n",
    "    appended = [start.append(x) for x in trgt_fst]\n",
    "    appended = [end.append(x) for x in trgt_lst]\n",
    "    periods = {\"ts\":range(len(start)), \"first\":start, \"last\":end}\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM data\n",
    "Utils for retrieving road data from OSM through its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order bbox from W,S,E,N to S,W,N,E\n",
    "def convert_bbox_osm(bbox):\n",
    "    offset = 0.05 # add a buffer to bbox in order to be sure cube is entirely covered\n",
    "    bbox_osm = [bbox[1], bbox[0], bbox[3], bbox[2]]\n",
    "    bbox_osm[0] -= offset # min lat\n",
    "    bbox_osm[1] -= offset # min lon\n",
    "    bbox_osm[2] += offset # max lat\n",
    "    bbox_osm[3] += offset # max lon\n",
    "    return bbox_osm\n",
    "\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_value String OSM value\n",
    "# osm_key String OSM key\n",
    "# element_type List of String\n",
    "# returns GeoPandasDataFrame\n",
    "def get_osm(bbox, \n",
    "            bbox_id,\n",
    "            osm_value = \"motorway\",\n",
    "            osm_key = \"highway\", # in OSM 'highway' contains several road types: https://wiki.openstreetmap.org/wiki/Key:highway\n",
    "            element_type = [\"way\", \"relation\"]):\n",
    "    \n",
    "    bbox_osm = convert_bbox_osm(bbox)\n",
    "    quot = '\"'\n",
    "    select = quot+osm_key+quot + '=' + quot+osm_value+quot\n",
    "    select_link = select.replace(osm_value, osm_value + \"_link\") # also get road links\n",
    "    select_junction = select.replace(osm_value, osm_value + \"_junction\")\n",
    "    geoms = []\n",
    "    for selector in [select, select_link, select_junction]:  \n",
    "        try:\n",
    "            query = overpassQueryBuilder(bbox=bbox_osm, \n",
    "                                         elementType=element_type, \n",
    "                                         selector=selector, \n",
    "                                         out='body',\n",
    "                                         includeGeometry=True)\n",
    "            elements = Overpass().query(query, timeout=60).elements()\n",
    "            # create multiline of all elements\n",
    "            if len(elements) > 0:\n",
    "                for i in range(len(elements)):\n",
    "                    elem = elements[i]\n",
    "                    geoms.append(elem.geometry())\n",
    "        except:\n",
    "            Warning(\"Could not retrieve \" + select)\n",
    "    try:\n",
    "        lines = gpd.GeoDataFrame(crs = EPSG_4326(), geometry = geoms)\n",
    "        n = len(geoms)\n",
    "        lines[BBOX_ID()] = [bbox_id]*n\n",
    "        lines[\"osm_value\"] = [osm_value]*n # add road type\n",
    "        return lines\n",
    "    except:\n",
    "        Warning(\"Could not merge \" + osm_value)\n",
    "        \n",
    "# buffer Float road buffer distance [m]\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_values List of String OSM values\n",
    "# osm_key String OSM key\n",
    "# roads_buffer Float buffer width\n",
    "# dir_write\n",
    "def get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dir_write):\n",
    "    fwrite = fname_osm(dir_write, bbox_id, osm_key)\n",
    "    if not os.path.exists(fwrite):\n",
    "        roads = []\n",
    "        has_error = []\n",
    "        offset = 0.00002\n",
    "        buffer_dist = \"buffer_distance\"\n",
    "        # buffer according to road type\n",
    "        m,t,p,s,ter = \"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\"\n",
    "        buffers = {m:roads_buffer, t:roads_buffer-offset, p:roads_buffer-(2*offset), s:roads_buffer-(3*offset), ter:roads_buffer-(4*offset)}\n",
    "        osm_values_int = {m:1, t:2, p:3, s:4, ter:5}\n",
    "        for osm_value in osm_values:\n",
    "            try:\n",
    "                roads_osm = get_osm(bbox = bbox, bbox_id = bbox_id, osm_value = osm_value)\n",
    "                roads_osm[buffer_dist] = [buffers[osm_value]] * len(roads_osm)\n",
    "                roads_osm[\"osm_value_int\"] = osm_values_int[osm_value]\n",
    "                roads.append(roads_osm)\n",
    "            except:\n",
    "                has_error.append(1)\n",
    "                Warning(\"'get_osm'\" + \"failed for bbox_id \"+ str(bbox_id) + \"osm_value \" + osm_value + \"osm_key\" + osm_key)\n",
    "        if len(roads) > len(has_error):\n",
    "            roads_merge = gpd.GeoDataFrame(pd.concat(roads, ignore_index=True), crs=roads[0].crs)\n",
    "            buffered = roads_merge.buffer(distance=roads_merge[buffer_dist])\n",
    "            roads_merge.geometry = buffered\n",
    "            roads_merge.to_file(fwrite, driver = GPKG())\n",
    "    return fwrite\n",
    "\n",
    "# osm geodataframe of polygons\n",
    "# reference_raster xarray with lat and lon\n",
    "def rasterize_osm(osm, reference_raster):\n",
    "    osm_values = list(set(osm[\"osm_value\"]))\n",
    "    nan_placeholder = 100\n",
    "    road_rasters = []\n",
    "    for osm_value in osm_values:\n",
    "        osm_subset = osm[osm[\"osm_value\"] == osm_value]\n",
    "        raster = rasterize(osm_subset, reference_raster.lat, reference_raster.lon)\n",
    "        cond = np.isfinite(raster)\n",
    "        raster_osm = np.where(cond, list(osm_subset.osm_value_int)[0], nan_placeholder) # use placeholder instead of nan first\n",
    "        raster_osm = raster_osm.astype(np.float)\n",
    "        road_rasters.append(raster_osm)        \n",
    "    # merge road types in one layer\n",
    "    road_raster_np = np.array(road_rasters).min(axis=0) # now use the lowest value (highest road level) because some intersect\n",
    "    road_raster_np[road_raster_np == nan_placeholder] = 0\n",
    "    return road_raster_np # 0=no_road 1=motorway, 2=trunk, ...\n",
    "\n",
    "def osm_values_to_name(values):\n",
    "    mapping = {1:\"Motorway\", 2:\"Trunk\", 3:\"Primary\", 4:\"Secondary\", 5:\"Tertiary\"}\n",
    "    return [mapping[value] for value in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Truck detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TruckDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruckDetector detects trucks at acquisition-level\n",
    "class TruckDetector():   \n",
    "    def __init__(self, band_stack):\n",
    "        self.band_stack = band_stack\n",
    "        is_none = band_stack is None\n",
    "        self.B02 = None if is_none else band_stack.B02 \n",
    "        self.B03 = None if is_none else band_stack.B03\n",
    "        self.B04 = None if is_none else band_stack.B04\n",
    "        self.B08 = None if is_none else band_stack.B08\n",
    "        self.B11 = None if is_none else band_stack.B11\n",
    "        self.no_truck_mask = None\n",
    "        self.trucks = None\n",
    "        \n",
    "    def set_trucks(self, trucks):\n",
    "        self.trucks = trucks # for reload option\n",
    "    \n",
    "    # Calculate a binary mask where pixels that are definitely no trucks are represented as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### max_ndvi Float above this val: no trucks. For Vegetation\n",
    "    ### max_ndwi Float above this val: no trucks. For Water\n",
    "    ### max_ndsi Float above this val: no_trucks. For Snow\n",
    "    ### min_rgb Float above this val: no_trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_blue Float above this val: no_trucks\n",
    "    ### max_green Float above this val: no trucks\n",
    "    ### max_red Float above this val: no trucks\n",
    "    ### min_b11 Float below this val: no trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_b11 Float below this val: no trucks. For bright (sealed) surfaces, e.g. buildings\n",
    "    def calc_no_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        B08 = self.B08\n",
    "        B11 = self.B11\n",
    "        min_rgb = thresholds[\"min_rgb\"]\n",
    "        max_blue = thresholds[\"max_blue\"]\n",
    "        max_green = thresholds[\"max_green\"]\n",
    "        max_red = thresholds[\"max_red\"]\n",
    "        ndvi_mask = ((B08 - B04) / (B08 + B04)) < thresholds[\"max_ndvi\"]\n",
    "        ndwi_mask = ((B02 - B11) / (B02 + B11)) < thresholds[\"max_ndwi\"]\n",
    "        ndsi_mask = ((B03 - B11) / (B03 + B11)) < thresholds[\"max_ndsi\"]\n",
    "        low_rgb_mask = (B02 > thresholds[\"min_blue\"]) * (B03 > min_rgb) * (B04 > min_rgb)\n",
    "        high_rgb_mask = (B02 < max_blue) * (B03 < max_green) * (B04 < max_red)\n",
    "        self.no_truck_mask = ndvi_mask * ndwi_mask * ndsi_mask * low_rgb_mask * high_rgb_mask\n",
    "    \n",
    "    # Calculate a binary mask where trucks are represented as 1 and no trucks as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### min_green_ratio Float, minimum value of blue-green ratio\n",
    "    ### min_red_ratio Float, minimum value of blue-red ratio\n",
    "    def detect_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        bg_ratio = (B02 - B03) / (B02 + B03)\n",
    "        br_ratio = (B02 - B04) / (B02 + B04)\n",
    "        bg_ratio_masked = bg_ratio * self.no_truck_mask\n",
    "        br_ratio_masked = br_ratio * self.no_truck_mask\n",
    "        bg = bg_ratio_masked > thresholds[\"min_blue_green_ratio\"]\n",
    "        br = br_ratio_masked > thresholds[\"min_blue_red_ratio\"]\n",
    "        bg_max = bg_ratio_masked < thresholds[\"max_blue_green_ratio\"]\n",
    "        br_max = br_ratio_masked < thresholds[\"max_blue_red_ratio\"]\n",
    "        self.trucks = bg * br * bg_max * br_max\n",
    "        \n",
    "    def filter_trucks(self):\n",
    "        self.trucks = filter_spatial_3x3_extended(self.trucks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 6 | Processing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PeriodProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PeriodProcessor processes a period of dates, represented in one cube\n",
    "class PeriodProcessor():\n",
    "    def __init__(self, start, end, bbox, bbox_id):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.bbox = bbox\n",
    "        self.bbox_id = bbox_id\n",
    "        self.cube = None\n",
    "        self.dates = None\n",
    "        self.lon_lat = None\n",
    "        self.mean_trucks = None\n",
    "        self.sum_trucks = None\n",
    "        self.sum_obs = None\n",
    "        self.trucks_points_result = None\n",
    "        self.osm_mask = None\n",
    "    \n",
    "    def get_cube(self, dataset, bands, tile_size, spatial_res, day_bins):\n",
    "        config = CubeConfig(dataset_name = dataset,\n",
    "                            band_names = bands,\n",
    "                            tile_size = tile_size,\n",
    "                            geometry = self.bbox,\n",
    "                            spatial_res = spatial_res,\n",
    "                            time_range = [self.start, self.end])\n",
    "        cube = open_cube(config)\n",
    "        self.cube = cube\n",
    "        self.dates = cube.time.values\n",
    "        self.lon_lat = {\"lon\":cube.lon.values, \"lat\":cube.lat.values}\n",
    "    \n",
    "    def calc_osm_mask(self, osm):\n",
    "        osm_raster = rasterize_osm(osm, self.cube.B02.sel(time = self.dates[0]))\n",
    "        self.osm_mask = create_xr_dataset(osm_raster, self.lon_lat, \"roadmask\")\n",
    "    \n",
    "    # Add from acquisition methods\n",
    "    def add_n_observations(self, no_clouds, max_observations):\n",
    "        obs = np.where(no_clouds == 1, 1, 0)\n",
    "        if self.sum_obs is None:\n",
    "            self.sum_obs = obs.copy()\n",
    "        else:\n",
    "            curr_obs = obs.copy()\n",
    "            # add observations only where max_observations not reached\n",
    "            curr_obs[self.sum_obs == max_observations] = 0\n",
    "            self.sum_obs += curr_obs\n",
    "    \n",
    "    def add_detections(self, trucks, max_observations):\n",
    "        if self.sum_trucks is None:\n",
    "            self.sum_trucks = trucks.copy()\n",
    "        else:\n",
    "            curr_trucks = trucks.copy()\n",
    "            # add detections only where max_observations not reached\n",
    "            curr_trucks[self.sum_obs == max_observations] = 0\n",
    "            self.sum_trucks += curr_trucks\n",
    "    \n",
    "    # Temporal summary methods\n",
    "    def sum_truck_n(self):\n",
    "        self.sum_trucks = np.array(self.detections).sum(axis=0)\n",
    "    \n",
    "    def sum_observations(self):\n",
    "        self.sum_obs = np.array(self.n_observations).sum(axis=0)\n",
    "    \n",
    "    def mask_sum_obs_to_trucks(self):\n",
    "        self.sum_obs[self.sum_trucks == 0] = 0\n",
    "    \n",
    "    def mean_truck_n(self):\n",
    "        self.mean_trucks = np.divide(self.sum_trucks, self.sum_obs)\n",
    "        self.mean_trucks[np.isnan(self.mean_trucks)] = 0\n",
    "    \n",
    "    def mask_osm_to_trucks(self, osm_mask):\n",
    "        self.osm_mask = self.osm_mask.roadmask.values\n",
    "        self.osm_mask[self.sum_trucks == 0] = 0\n",
    "            \n",
    "    # Write methods\n",
    "    def write_n_observations(self, fname):\n",
    "        sum_obs_xr = create_xr_dataset(self.sum_obs, self.lon_lat, os.path.basename(fname))\n",
    "        sum_obs_xr.to_netcdf(fname)\n",
    "        \n",
    "    def write_sum_trucks(self, fname):\n",
    "        sum_xr = create_xr_dataset(period.sum_trucks, period.lon_lat, os.path.basename(fname))\n",
    "        sum_xr.to_netcdf(fname)\n",
    "    \n",
    "    def write_mean_trucks(self, fname):\n",
    "        mean_xr = create_xr_dataset(self.mean_trucks, self.lon_lat, os.path.basename(fname))\n",
    "        mean_xr.to_netcdf(fname)\n",
    "        \n",
    "    def write_trucks_vec(self, points, fname, fname_placeholder):\n",
    "        got_points = points is not None and len(points) > 0\n",
    "        if got_points:\n",
    "            points.to_file(fname, driver = vec_driver_from_ext(\".\" + fname.split(\".\")[1]))\n",
    "        else:\n",
    "            # write txt as placeholder\n",
    "            with open(fname_placeholder, \"w\") as file:\n",
    "                file.write(\"%s has length: %s. Nothing to write\" %(os.path.basename(fname_placeholder), str(len(points))))\n",
    "        \n",
    "    def wrap_period(self, fnames):\n",
    "        crs = EPSG_4326()\n",
    "        ind_right = \"index_right\"\n",
    "        #self.sum_observations()\n",
    "        self.write_n_observations(fnames[N_OBS()])\n",
    "        #self.sum_truck_n()\n",
    "        self.mean_truck_n()\n",
    "        self.mask_osm_to_trucks(self.osm_mask) # mask to trucks\n",
    "        self.mask_sum_obs_to_trucks() # mask to trucks\n",
    "        # merge n observations, n trucks and mean trucks into single points layer\n",
    "        try:\n",
    "            n_obs_points = raster_to_points(self.sum_obs, self.lon_lat, \"sum_observations\", crs)\n",
    "            sum_trucks_points = raster_to_points(self.sum_trucks, self.lon_lat, \"sum_trucks_sub_period\", crs)\n",
    "            sum_trucks_obs = gpd.sjoin(sum_trucks_points, n_obs_points, how=\"inner\", op=\"intersects\")\n",
    "            sum_trucks_obs = sum_trucks_obs.drop([ind_right], axis=1)\n",
    "            osm_points = raster_to_points(self.osm_mask, self.lon_lat, \"osm_value\", crs)\n",
    "            osm_points[\"osm_name\"] = osm_values_to_name(osm_points[\"osm_value\"])\n",
    "            sum_trucks_obs_osm = gpd.sjoin(sum_trucks_obs, osm_points, how=\"inner\", op=\"intersects\")\n",
    "            sum_trucks_obs_osm = sum_trucks_obs_osm.drop([ind_right], axis=1)\n",
    "            mean_trucks_points = raster_to_points(self.mean_trucks, self.lon_lat, \"mean_trucks\", crs)\n",
    "            self.trucks_points_result = gpd.sjoin(sum_trucks_obs_osm, mean_trucks_points, how=\"inner\", op=\"intersects\")\n",
    "            self.trucks_points_result = self.trucks_points_result.drop([ind_right], axis=1)\n",
    "            self.write_trucks_vec(self.trucks_points_result, fnames[TRUCKS_VEC()], fnames[TRUCKS_VEC_PHR()])\n",
    "        except:\n",
    "            Warning(\"No results could be written, detections could be empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AcquisitionProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AcquisitionProcessor processes all valid pixels of a single acquisition in cube\n",
    "class AcquisitionProcessor():\n",
    "    def __init__(self, date_np64, cube):\n",
    "        self.date_np64 = date_np64\n",
    "        self.cube = cube\n",
    "        self.band_stack = cube.sel(time = date_np64)\n",
    "        self.detector = None\n",
    "        self.no_clouds = None\n",
    "        self.osm_mask = None\n",
    "        self.has_obs = None\n",
    "           \n",
    "    def mask_clouds(self, cloud_masking_thresholds):       \n",
    "        scl = MaskSet(self.band_stack.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        cirrus = scl.cirrus\n",
    "        shadows = scl.cloud_shadows\n",
    "        no_data = scl.no_data\n",
    "        rgb_cloud_mask = calc_rgb_cloud_mask(self.band_stack, cloud_masking_thresholds)\n",
    "        self.no_clouds = (high_prob + med_prob + cirrus + shadows + no_data + rgb_cloud_mask) == 0\n",
    "        self.band_stack = self.band_stack.where(self.no_clouds)\n",
    "        \n",
    "    # percentage Float secifies the percentage of valid observations\n",
    "    # that is needed to be considered as valid acquisition\n",
    "    def has_observations(self, minimum_valid_percentage):\n",
    "        values = self.no_clouds.values\n",
    "        # mask valid pixel mask to OSM roads\n",
    "        values[self.osm_mask.roadmask == 0] = 0\n",
    "        n_vals = np.count_nonzero(self.osm_mask.roadmask)\n",
    "        n_valid = np.count_nonzero(values)\n",
    "        percent_valid = (n_valid / n_vals) * 100\n",
    "        self.has_obs = percent_valid >= minimum_valid_percentage\n",
    "    \n",
    "    # osm gpd polygons\n",
    "    def mask_with_osm(self, osm_mask):\n",
    "        self.osm_mask = osm_mask\n",
    "        self.band_stack = self.band_stack.where(self.osm_mask.roadmask != 0)\n",
    "                \n",
    "    def do_detection(self, thresholds):\n",
    "        self.detector = TruckDetector(self.band_stack)\n",
    "        self.detector.calc_no_trucks(thresholds)\n",
    "        self.detector.detect_trucks(thresholds)\n",
    "        self.detector.filter_trucks()\n",
    "        \n",
    "    def write_detections(self, fname):\n",
    "        trucks_xr = create_xr_dataset(self.detector.trucks, {\"lat\":self.cube.lat.values, \"lon\":self.cube.lon.values},\n",
    "                                    os.path.basename(fname))\n",
    "        # add no_clouds layer (n observations) for possible reload\n",
    "        trucks_xr_with_no_clouds = trucks_xr.assign({\"no_clouds\":self.no_clouds})\n",
    "        trucks_xr_with_no_clouds.to_netcdf(fname)\n",
    "        \n",
    "    def read_detections(self, fname):\n",
    "        dataset = xr.open_dataset(fname)\n",
    "        self.no_clouds = dataset[\"no_clouds\"]\n",
    "        trucks = dataset[os.path.basename(fname)]\n",
    "        self.detector = TruckDetector(None)\n",
    "        self.detector.set_trucks(trucks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "____________\n",
    "## 7 | Execute processing\n",
    "Process by grid __box__ (bbox_id), __sub-period__, __acquisition__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "====================================================================================================\n",
      "\n",
      "Starting truck detection processing \n",
      "..................................................\n",
      "\n",
      "Weekdays: ['tuesday', 'wednesday', 'thursday']\n",
      "Number of grid cells to process: 9\n",
      "Number of periods: 4\n",
      "Timestamps sub-period: 1\n",
      "OSM roads: ['motorway', 'trunk', 'primary']\n",
      "Baseline years: [2017, 2018, 2019]\n",
      "Minimum valid observations to consider acquisition: 20 %\n",
      "Overwrite results: True\n",
      "..................................................\n",
      "\n",
      "====================================================================================================\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing: 48\n",
      "bbox_id: 834\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"motorway\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"motorway\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"motorway_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"motorway_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"motorway_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"motorway_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] waiting for 10.0 seconds\n",
      "[overpass] waiting for 5.0 more seconds\n",
      "[overpass] start processing\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"trunk\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"trunk\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] waiting for 1.0 seconds\n",
      "[overpass] start processing\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"trunk_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"trunk_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"trunk_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"trunk_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] waiting for 2.0 seconds\n",
      "[overpass] start processing\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"primary\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"primary\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] waiting for 1.0 seconds\n",
      "[overpass] start processing\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"primary_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"primary_link\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] downloading data: [timeout:60][out:json];(way[\"highway\"=\"primary_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);relation[\"highway\"=\"primary_junction\"](48.85298222767442,6.890160046307692,49.96563083232557,7.995642852);); out body geom;\n",
      "[overpass] waiting for 3.0 seconds\n",
      "[overpass] start processing\n",
      "TS: 0 Period Start: 2017-03-16 00:00:00 End: 2017-06-16 00:00:00\n",
      "Getting cube\n",
      "Calculating OSM mask\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Cloud masking failed. Assuming not enough observations\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Cloud masking failed. Assuming not enough observations\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Cloud masking failed. Assuming not enough observations\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Cloud masking failed. Assuming not enough observations\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating cloud mask\n",
      "--------------------------------------------------------------------------------\n",
      "Cloud masking failed. Assuming not enough observations\n",
      "Skipping, valid obs. below 20 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Processing sub-period took: 1.8666666666666667 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Done with period 0 of bbox_id 834\n",
      "--------------------------------------------------------------------------------\n",
      "TS: 1 Period Start: 2018-03-16 00:00:00 End: 2018-06-16 00:00:00\n",
      "Getting cube\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8f03eb188666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting cube\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mperiod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeriodProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# track time cube is open to prevent timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosm_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-5be23e4bda4e>\u001b[0m in \u001b[0;36mget_cube\u001b[0;34m(self, dataset, bands, tile_size, spatial_res, day_bins)\u001b[0m\n\u001b[1;32m     24\u001b[0m                             \u001b[0mspatial_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             time_range = [self.start, self.end])\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mcube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/cube.py\u001b[0m in \u001b[0;36mopen_cube\u001b[0;34m(cube_config, observer, trace_store_calls, max_cache_size, **sh_kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0msentinel_hub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentinelHub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msh_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mcube_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentinelHubStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentinel_hub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_store_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace_store_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_cache_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcube_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLRUStoreCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcube_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cache_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/store.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentinel_hub, cube_config, observer, trace_store_calls)\u001b[0m\n\u001b[1;32m    437\u001b[0m         super().__init__(cube_config,\n\u001b[1;32m    438\u001b[0m                          \u001b[0mobserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                          trace_store_calls=trace_store_calls)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_time_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/store.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cube_config, observer, trace_store_calls)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobserver\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobserver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_store_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_store_calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_ranges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_time_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_ranges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/store.py\u001b[0m in \u001b[0;36mget_time_ranges\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m                                                              \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cube_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                                                              time_range=(time_start.strftime(\"%Y-%m-%d\"),\n\u001b[0;32m--> 454\u001b[0;31m                                                                          time_end.strftime(\"%Y-%m-%d\")))\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_features_to_time_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/sentinelhub.py\u001b[0m in \u001b[0;36mget_tile_features\u001b[0;34m(self, feature_type_name, bbox, time_range)\u001b[0m\n\u001b[1;32m    137\u001b[0m                                         \u001b[0mfeature_type_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_type_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                                         \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                         time_range=time_range)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xcube_sh/sentinelhub.py\u001b[0m in \u001b[0;36mfetch_tile_features\u001b[0;34m(cls, instance_id, feature_type_name, bbox, time_range)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mquery_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURE_OFFSET\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'http://services.sentinel-hub.com/ogc/wfs/{instance_id}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make or read processing grid\n",
    "if os.path.exists(files[\"proc_grid\"]):\n",
    "    try:\n",
    "        grid_gadm = gpd.read_file(files[\"proc_grid\"])\n",
    "    except:\n",
    "        raise Exception(\"Failed reading proc grid from: \" + files[\"proc_grid\"])\n",
    "else:\n",
    "    grid_gadm = make_grid(grid_spacing, files)\n",
    "if country is not None:\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"NAME_0\"]==country]\n",
    "# calc temporal bounds of baseline sub-periods\n",
    "if len(baseline_years) > 0:\n",
    "    periods = yearly_period_from_target(target, baseline_years, timestamps_sub_period)\n",
    "else:\n",
    "    periods = calc_periods(n_days_sub, baseline, target)\n",
    "print(\"-\" * 30, )\n",
    "trace = []\n",
    "ext_arr = NC_EXT()\n",
    "ext_vec = GPKG_EXT()\n",
    "sep = \"-\" * 80\n",
    "n_boxes = len(grid_gadm)\n",
    "process_ids = [834, 836, 838, 789, 790, 791, 881, 877, 879]\n",
    "n_boxes = len(process_ids) ###########################\n",
    "start_msg(weekdays, n_boxes, periods, timestamps_sub_period, osm_values, baseline_years, minimum_valid_observations, overwrite_results)\n",
    "for i in range(n_boxes):\n",
    "    i = list(grid_gadm.bbox_id).index(process_ids[i])\n",
    "    bbox = list(grid_gadm.geometry)[i].bounds\n",
    "    bbox_id = list(grid_gadm[BBOX_ID()])[i]\n",
    "    bbox_id_str = str(bbox_id)\n",
    "    first, last = periods[\"first\"], periods[\"last\"]\n",
    "    print(\"%s\\n\\nProcessing: %s\\nbbox_id: %s\\n\\n%s\" %(sep, str(i), bbox_id_str, sep))\n",
    "    try:\n",
    "        file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        # retry\n",
    "        if not os.path.exists(file_osm): file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        osm = gpd.read_file(file_osm)\n",
    "        # subset according to desired osm road levels (file might have been written with other values)\n",
    "        value_not_desired = [v not in osm_values for v in list(osm[\"osm_value\"])]\n",
    "        osm = osm.drop(osm[value_not_desired].index)\n",
    "    except:\n",
    "        msg = \"Could not get OSM roads: \" + bbox_id_str\n",
    "        Warning(msg)\n",
    "        trace.append(msg)\n",
    "        #continue\n",
    "    for start, end in zip(first, last):\n",
    "        period_files = []\n",
    "        tracker_start = datetime.now()\n",
    "        ts = first.index(start)\n",
    "        ts_str = str(ts)\n",
    "        dir_ts = dirs[\"processed\"] # write into output pool\n",
    "        if not os.path.exists(dir_ts): os.mkdir(dir_ts)\n",
    "        dirs_ts = make_dirs_ts(dir_ts)\n",
    "        print(\"TS: %s Period Start: %s End: %s\" %(ts_str, str(start), str(end)))\n",
    "        \n",
    "        # check if yet processed\n",
    "        fnames = sub_period_fnames(dirs_ts, bbox_id, str(start.date()) + \"_\" + str(end.date()), ext_arr, ext_vec)\n",
    "        exists = {}\n",
    "        for key, f in fnames.items():\n",
    "            exists[key] = os.path.exists(f) if not f[-3:]==\"txt\" else True       \n",
    "        if not exists[TRUCKS_VEC()]: exists[TRUCKS_VEC()] = os.path.exists(fnames[TRUCKS_VEC_PHR()]) # placeholder might exist instead\n",
    "        already_proc = \"because already processed\"\n",
    "        osm_msg = \"Calculating OSM mask\\n\" + sep\n",
    "        skip = all(exists.values()) and not overwrite_results\n",
    "        if skip:\n",
    "            print(\"Skipping \" + already_proc)\n",
    "        else:\n",
    "            print(\"Getting cube\")\n",
    "            period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "            period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "            t1 = datetime.now() # track time cube is open to prevent timeout\n",
    "            print(osm_msg)\n",
    "            period.calc_osm_mask(osm)\n",
    "            for period_date in period.dates:\n",
    "                if (datetime.now()-t1).seconds > 1800: # if 30 minutes exceeded get cube again to prevent timeout\n",
    "                    print(\"Getting cube again to prevent timeout\")\n",
    "                    period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "                    period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "                    t1 = datetime.now()\n",
    "                    print(osm_msg)\n",
    "                    period.calc_osm_mask(osm)\n",
    "                acquisition = AcquisitionProcessor(period_date, period.cube)\n",
    "                date_str = str(period_date).replace(\":\",\"_\")[0:-10]\n",
    "                fname = fname_acquisition_trucks(dirs_ts, bbox_id, date_str, ext_arr)\n",
    "                if os.path.exists(fname):\n",
    "                    print(\"File already processed\\n%s\" %(sep))\n",
    "                    period_files.append(fname)\n",
    "                else:\n",
    "                    weekday_match = any([is_weekday(period_date, w) for w in weekdays])\n",
    "                    #weekday_match = False ###################################\n",
    "                    if weekday_match:\n",
    "                        try:\n",
    "                            min_valid_str = str(minimum_valid_observations)\n",
    "                            fname_placeholder = os.path.join(dirs_ts[\"acquisitions_trucks\"], \n",
    "                                                             min_valid_str + date_str + \"_placeholder.txt\").replace(\"-\",\"_\")\n",
    "                            can_have_obs = not os.path.exists(fname_placeholder)\n",
    "                            if can_have_obs:\n",
    "                                print(\"Calculating cloud mask\\n%s\" %(sep))\n",
    "                                acquisition.mask_clouds(cloud_masking_thresholds)\n",
    "                                acquisition.mask_with_osm(period.osm_mask) # mask to OSM roads\n",
    "                                # check if enough observations to be considered\n",
    "                                acquisition.has_observations(minimum_valid_observations)\n",
    "                                if not acquisition.has_obs:\n",
    "                                    with open(fname_placeholder, \"w\") as f:\n",
    "                                        f.write(\"%s not fulfilled for date %s\" %(min_valid_str, date_str))\n",
    "                            else:\n",
    "                                has_obs = can_have_obs\n",
    "                        except:\n",
    "                            print(\"Cloud masking failed. Assuming not enough observations\")\n",
    "                            acquisition.has_obs = False\n",
    "                        if acquisition.has_obs:\n",
    "                            print(\"Processing acquisition: %s\" %(date_str))\n",
    "                            period_files.append(fname)\n",
    "                            try:\n",
    "                                acquisition.do_detection(thresholds) # truck detection\n",
    "                                print(\"Writing\")\n",
    "                                acquisition.write_detections(fname)\n",
    "                                print(\"Done\")\n",
    "                            except:\n",
    "                                print(\"Failed: %s\" %(date_str))\n",
    "                            print(\"Done with acquisition: %s\\n%s\" %(date_str, sep))\n",
    "                        else:\n",
    "                            print(\"Skipping, valid obs. below %s %%\\n%s\" %(minimum_valid_observations, sep))\n",
    "            if len(period_files) > 0:\n",
    "                print(\"%s\\nNumber of files: %s\" %(sep, str(len(period_files))))\n",
    "                # read all processed files from disk in order not to hold in memory during processing\n",
    "                print(\"Aggregating sub-period\")\n",
    "                for file in period_files:\n",
    "                    if os.path.exists(file):\n",
    "                        try:\n",
    "                            print(\"Reading file: %s\" %(file))\n",
    "                            acquisition.read_detections(file)\n",
    "                            period.add_n_observations(acquisition.no_clouds, max_observations)\n",
    "                            period.add_detections(acquisition.detector.trucks.values, max_observations)\n",
    "                        except:\n",
    "                            Warning(\"Could not read: \" + fname)\n",
    "                    else:\n",
    "                        print(\"File does not exist: %s\" %(file))\n",
    "                period.wrap_period(fnames)\n",
    "            else:\n",
    "                msg = \"No acquisitions in period %s to %s. In bbox_id: %s\" %(str(start), str(end), bbox_id_str)\n",
    "                Warning(msg)\n",
    "                trace.append(msg)\n",
    "            print(\"%s\\nProcessing sub-period took: %s minutes\" %(sep, str((datetime.now()-tracker_start).seconds/60)))\n",
    "        print(\"%s\\nDone with period %s of bbox_id %s\\n%s\" %(sep, ts_str, bbox_id_str, sep))    \n",
    "    print(\"%s\\nDone with bbox_id: %s\\n%s\\n%s\" %(sep, bbox_id_str, sep, sep))\n",
    "print(\"%s\\nDone with all requested bboxes\\n%s\\n%s\" %(sep, sep, sep*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-period-wise read points layer of bounding boxes\n",
    "# intersect layers country-wise with country borders\n",
    "# write statistics country-wise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
