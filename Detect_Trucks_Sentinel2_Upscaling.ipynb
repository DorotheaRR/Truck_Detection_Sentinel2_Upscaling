{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Trucks Sentinel-2 - Europe\n",
    "________________                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# load creds\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [48.20, 16.30, 48.30, 16.46]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 1 | Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# OSM API\n",
    "from OSMPythonTools.overpass import overpassQueryBuilder, Overpass\n",
    "\n",
    "# xcube\n",
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "# spatial\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely import geometry, coords\n",
    "from shapely.geometry import Polygon\n",
    "#from osgeo import gdal #, gdal_array, ogr\n",
    "\n",
    "# plotting\n",
    "import matplotlib as plt\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main = os.getcwd()\n",
    "dir_not_commit = os.path.join(dir_main, \"not_commit\")\n",
    "dir_ancil = os.path.join(dir_not_commit, \"ancillary_data\")\n",
    "dirs = {\"dir\":dir_main, \"dir_not_commit\":dir_not_commit, \"ancil\":dir_ancil, \"processing\":os.path.join(dir_main, \"processing\"),\n",
    "       \"processed\":os.path.join(dir_not_commit, \"processed\"), \"ancil_roads\":os.path.join(dir_ancil, \"roads\"), \"ancil_gadm\":os.path.join(dir_ancil, \"gadm\")}\n",
    "for directory in list(dirs.values()):\n",
    "    if not os.path.exists(directory): os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\"gadm_efta\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_EU_EFTA.gpkg\"), \n",
    "         \"gadm_europe\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_europe.gpkg\"), \n",
    "         \"gadm_europe_union\":os.path.join(dirs[\"ancil_gadm\"], \"gadm0_europe_union.gpkg\"),\n",
    "         \"proc_grid\":os.path.join(dirs[\"processing\"], \"processing_grid_gadm.geojson\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2 | General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_results = True # shall already existing results be overwritten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # 10m\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"SCL\"]\n",
    "day_bins = \"1D\" # for cube\n",
    "minimum_valid_observations = 15\n",
    "grid_spacing = 1. # processing grid box size [degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = \"wednesday\" # process for this weekday\n",
    "target = {\"first\":datetime(2020, 3, 16), \"last\":datetime(2020, 6, 6)}\n",
    "baseline = {\"first\":datetime(2017, 12, 15), \"last\":datetime(2020, 3, target[\"first\"].day - 1)}\n",
    "n_days_sub = 91 # days per timestamp (sub-period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_key = \"highway\"\n",
    "osm_values = [\"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\"]\n",
    "roads_buffer = 0.001 # degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 3 | Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\"min_rgb\":0.04,\n",
    "              \"max_red\":0.15,\n",
    "              \"max_green\":0.15,\n",
    "              \"max_blue\":0.4,\n",
    "              \"max_ndvi\":0.7,\n",
    "              \"max_ndwi\":0.001,\n",
    "              \"max_ndsi\":0.0001,\n",
    "              \"min_b11\":0.05,\n",
    "              \"max_b11\":0.55,\n",
    "              \"min_green_ratio\":0.05,\n",
    "              \"min_red_ratio\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## 4 | Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPSG_4326(): return \"EPSG:4326\"\n",
    "def EPSG_3857(): return \"EPSG:3857\"\n",
    "def GEOJSON(): return \"GeoJSON\"\n",
    "def GPKG(): return \"GPKG\"\n",
    "def GEOJSON_EXT(): return \".geojson\"\n",
    "def GPKG_EXT(): return \".gpkg\"\n",
    "def NC_EXT(): return \".nc\"\n",
    "def BBOX_ID(): return(\"bbox_id\")\n",
    "def ABS_N_TRUCKS(): return(\"abs_n_trucks\")\n",
    "def SUM_TRUCKS(): return(\"sum_trucks\")\n",
    "def MEAN_N_TRUCKS(): return(\"mean_n_trucks\")\n",
    "def MEAN_N_TRUCKS_VEC(): return(\"mean_n_trucks_vec\")\n",
    "def N_OBS(): return(\"n_obs_fname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname_osm(bbox_id, osm_key, ext = GEOJSON_EXT()): return str(bbox_id) + \"_\" + osm_key + ext\n",
    "def fname_abs_n_trucks(dirs_ts, bbox_id, ext = NC_EXT()): return os.path.join(dirs_ts[ABS_N_TRUCKS()], bbox_id + ext)\n",
    "def fname_sum_trucks(dirs_ts, bbox_id, ext = NC_EXT()): return os.path.join(dirs_ts[SUM_TRUCKS()], bbox_id + ext)\n",
    "def fname_mean_trucks(dirs_ts, bbox_id, ext = NC_EXT()): return os.path.join(dirs_ts[MEAN_N_TRUCKS()], bbox_id + ext)\n",
    "def fname_mean_trucks_vec(dirs_ts, bbox_id, ext = GPKG_EXT()): return os.path.join(dirs_ts[MEAN_N_TRUCKS_VEC()], bbox_id + ext)\n",
    "def fname_sum_obs(dirs_ts, bbox_id, ext = NC_EXT()): return os.path.join(dirs_ts[N_OBS], bbox_id + ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_ts(dir_ts):\n",
    "    dir_ts_overall = os.path.join(dir_ts, \"overall\")\n",
    "    dirs_ts = {ABS_N_TRUCKS():os.path.join(dir_ts, ABS_N_TRUCKS()), # acquisition-wise\n",
    "               SUM_TRUCKS():os.path.join(dir_ts_overall, SUM_TRUCKS()) # aggregation\n",
    "               MEAN_N_TRUCKS():os.path.join(dir_ts_overall, MEAN_N_TRUCKS()), # aggr\n",
    "               MEAN_N_TRUCKS_VEC():os.path.join(dir_ts_overall, MEAN_N_TRUCKS_VEC()), # aggr\n",
    "               N_OBS():os.path.join(dir_ts_overall, N_OBS())} # aggr\n",
    "    for direc in dirs_ts.values():\n",
    "        if not os.path.exists(direc): os.mkdir(direc)\n",
    "    return dirs_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing grid\n",
    "Create a grid covering Europe for processing chunk-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(grid_spacing, files):\n",
    "    crs = EPSG_4326()\n",
    "    gadm0_eu_efta = gpd.read_file(files[\"gadm_efta\"])\n",
    "    xmin,ymin,xmax,ymax = gadm0_eu_efta.total_bounds\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    cols = int((width) / grid_spacing)\n",
    "    rows = int((height) / grid_spacing)\n",
    "    box_width = width / cols\n",
    "    box_height = height / rows\n",
    "    boxes = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            col_right = col + 1\n",
    "            row_lower = row + 1\n",
    "            y_up = ymax-row*box_height\n",
    "            y_low = ymax-row_lower*box_height\n",
    "            x_left = xmin+col*box_width\n",
    "            x_right = xmin+col_right*box_width\n",
    "            boxes.append(Polygon([(x_left, y_up), \n",
    "                                  (x_right, y_up), \n",
    "                                  (x_right, y_low), \n",
    "                                  (x_left, y_low)]))\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":boxes})\n",
    "    grid.crs = crs\n",
    "    gadm0_europe = gpd.read_file(files[\"gadm_europe\"])\n",
    "    gadm0_europe.crs = crs\n",
    "    file_union = files[\"gadm_europe_union\"]\n",
    "    if os.path.exists(file_union):\n",
    "        gadm0_europe_clip_union = gpd.read_file(file_union)\n",
    "    else:\n",
    "        gadm0_europe[\"continent\"] = [\"EUROPE\"] * len(gadm0_europe)\n",
    "        gadm0_europe_clip = gpd.overlay(gadm0_europe, gpd.GeoDataFrame({\"a\":[1],\"geometry\":test}), how = \"intersection\")\n",
    "        gadm0_europe_clip_union = gadm0_europe_clip.dissolve(by = \"continent\")\n",
    "        gadm0_europe_clip_union.to_file(file_union, driver = GPKG())\n",
    "    # intersect with gadm for cutting boxes at coast line\n",
    "    grid_intersect = gpd.overlay(grid, gadm0_europe_clip_union, how = \"intersection\")\n",
    "    # get country attributes\n",
    "    grid_gadm = gpd.sjoin(grid_intersect, gadm0_europe, how = \"left\", op = \"intersects\")\n",
    "    grid_gadm[BBOX_ID()] = range(len(grid_gadm))\n",
    "    boxes = grid_gadm.geometry.apply(lambda geom : geom.bounds)\n",
    "    grid_gadm.geometry = [Polygon(geometry.box(b[0], b[1], b[2], b[3])) for b in boxes] # use the light bboxes\n",
    "    delete_cols = [\"GID_0_left\", \"NAME_0_left\", \"a\", \"continent\", \"index_right\"]\n",
    "    for column in delete_cols:\n",
    "        if column in grid_gadm.columns: \n",
    "            grid_gadm = grid_gadm.drop(column, 1)\n",
    "    grid_gadm = grid_gadm.rename(columns = {\"GID_0_right\":\"GID_0\", \"NAME_0_right\":\"NAME_0\"})\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"GID_0\"] != \"RUS\"] # do not include\n",
    "    grid_gadm.to_file(files[\"proc_grid\"], driver = GEOJSON())\n",
    "    return grid_gadm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data np array\n",
    "# lon_lat dict of:\n",
    "### \"lon\": np array longitude values\n",
    "### \"lat\": np array latitude values\n",
    "def create_xy_xarray(data, lon_lat):\n",
    "    return xr.DataArray(data, coords = lon_lat, dims = (\"lat\", \"lon\"))\n",
    "\n",
    "# extracts coordinates at value in np array and returns points as GeoDataFrame\n",
    "# data 2d np array\n",
    "# match_value Float value in data where point coordinates are extracted\n",
    "# lon_lat dict of:\n",
    "### \"lon\": np array longitude values\n",
    "### \"lat\": np array latitude values\n",
    "def points_from_np(data, match_value, lon_lat):\n",
    "    indices = np.argwhere(data == match_value)\n",
    "    if len(indices) > 0:    \n",
    "        lat_indices = indices[,[0]]\n",
    "        lon_indices = indices[,[1]]\n",
    "        lat_coords = lon_lat[\"lat\"][lat_indices]\n",
    "        lon_coords = lon_lat[\"lon\"][lon_indices]\n",
    "        points = gpd.GeoDataFrame(geometry = gpd.points_from_xy(lon_coords, lat_coords))\n",
    "        return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date Datetime object to be checked\n",
    "# weekday String weekday to be checked against\n",
    "# returns Boolean if date is weekday\n",
    "def is_weekday(date, weekday):\n",
    "    weekday = weekday.lower()[0:3]\n",
    "    y, m, d = 2000, 1, 3\n",
    "    wd = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "    ref = {}\n",
    "    for i in range(len(wd)): ref[wd[i]] = datetime(y, m, d + i)        \n",
    "    return ((date - ref[weekday]) % 7) == 0 # check if date is in a sequence of 7\n",
    "\n",
    "# Calculates the dates of the periods representing a timestamp each\n",
    "# n_days_sub Integer how many dates shall one timestamp (sub-period) cover\n",
    "# baseline Dict start and end dates overall baseline period\n",
    "# target Dict start and end dates target period\n",
    "def calc_periods(n_days_sub, baseline, target):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    base_first = baseline[fst]\n",
    "    n_days = target[fst] - base_first\n",
    "    n_sub = int(n_days.days / n_days_sub) # number of timestamps\n",
    "    start = [base_first] * n_sub\n",
    "    start = [start[i] + timedelta(n_days_sub * i) for i in range(len(start))]\n",
    "    end = [start[i+1] - timedelta(1) for i in range(len(start)-1)]\n",
    "    end.append(baseline[lst])\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    return periods\n",
    "\n",
    "# formats a date for use in xcube\n",
    "def datetime_to_np(datetime):\n",
    "    return np.datetime64(datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM data\n",
    "Utils for retrieving road data from OSM through its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_value String OSM value\n",
    "# osm_key String OSM key\n",
    "# element_type List of String\n",
    "# returns GeoPandasDataFrame\n",
    "def get_osm(bbox, \n",
    "            bbox_id,\n",
    "            osm_value = \"motorway\",\n",
    "            osm_key = \"highway\", # in OSM 'highway' contains several road types: https://wiki.openstreetmap.org/wiki/Key:highway\n",
    "            element_type = [\"way\", \"relation\"]):\n",
    "    \n",
    "    quot = '\"'\n",
    "    select = quot+osm_key+quot + '=' + quot+osm_value+quot\n",
    "    select_link = select.replace(osm_value, osm_value + \"_link\") # also get road links\n",
    "    geoms = []\n",
    "    for selector in [select, select_link]:  \n",
    "        try:\n",
    "            query = overpassQueryBuilder(bbox=bbox, \n",
    "                                         elementType=element_type, \n",
    "                                         selector=selector, \n",
    "                                         out='body',\n",
    "                                         includeGeometry=True)\n",
    "            elements = Overpass().query(query).elements()\n",
    "            # create multiline of all elements\n",
    "            if len(elements) > 0:\n",
    "                for i in range(len(elements)):\n",
    "                    elem = elements[i]\n",
    "                    if elem.tags()[osm_key] == osm_value: geoms.append(elem.geometry())\n",
    "        except:\n",
    "            Warning(\"Could not retrieve \" + select)\n",
    "    try:\n",
    "        lines = gpd.GeoDataFrame(crs = EPSG_4326(), geometry = geoms)\n",
    "        n = len(geoms)\n",
    "        lines[BBOX_ID()] = [bbox_id]*n\n",
    "        lines[\"osm_value\"] = [osm_value]*n # add road type\n",
    "        return lines\n",
    "    except:\n",
    "        Warning(\"Could not merge \" + osm_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer Float road buffer distance [m]\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_values List of String OSM values\n",
    "# osm_key String OSM key\n",
    "# buffer Float buffer width\n",
    "# dir_write\n",
    "def get_roads(bbox, bbox_id, osm_values, osm_key, buffer, dir_write):\n",
    "    fwrite = os.path.join(dir_write, fname_osm(bbox_id, osm_key))\n",
    "    roads = []\n",
    "    for osm_value in osm_values:\n",
    "        try:\n",
    "            roads_osm = get_osm(bbox = bbox, bbox_id = bbox_id, osm_value = osm_value)\n",
    "        except:\n",
    "            Warning(\"'get_osm'\" + \"failed for bbox_id \"+ str(bbox_id) + \"osm_value \" + osm_value + \"osm_key\" + osm_key)\n",
    "        roads.append(roads_osm)\n",
    "    roads_merge = gpd.GeoDataFrame(pd.concat(roads, ignore_index=True), crs=roads[0].crs)\n",
    "    roads_buff = roads_merge.buffer(buffer)\n",
    "    roads_buff.to_file(fwrite, driver = GEOJSON())\n",
    "    return fwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Truck detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruckDetector():\n",
    "    def __init__(self, band_stack):\n",
    "        self.band_stack = band_stack\n",
    "        self.B02 = band_stack.B02\n",
    "        self.B03 = band_stack.B03\n",
    "        self.B04 = band_stack.B04\n",
    "        self.B08 = band_stack.B08\n",
    "        self.B11 = band_stack.B11\n",
    "        self.no_truck_mask = None # xarray\n",
    "        self.trucks = None # xarray\n",
    "    \n",
    "    # Calculate a binary mask where pixels that are definitely no trucks are represented as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### max_ndvi Float above this val: no trucks. For Vegetation\n",
    "    ### max_ndwi Float above this val: no trucks. For Water\n",
    "    ### max_ndsi Float above this val: no_trucks. For Snow\n",
    "    ### min_rgb Float above this val: no_trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_blue Float above this val: no_trucks\n",
    "    ### max_green Float above this val: no trucks\n",
    "    ### max_red Float above this val: no trucks\n",
    "    ### min_b11 Float below this val: no trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_b11 Float below this val: no trucks. For bright (sealed) surfaces, e.g. buildings\n",
    "    def calc_no_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        B08 = self.B08\n",
    "        B11 = self.B11\n",
    "        min_rgb = thresholds[\"min_rgb\"]\n",
    "        max_blue = thresholds[\"max_blue\"]\n",
    "        max_green = thresholds[\"max_green\"]\n",
    "        max_red = thresholds[\"max_red\"]\n",
    "        max_b11 = thresholds[\"max_b11\"]\n",
    "        ndvi_mask = ((B08 - B04) / (B08 + B04)) < thresholds[\"max_ndvi\"]\n",
    "        ndwi_mask = ((B02 - B11) / (B02 + B11)) < thresholds[\"max_ndwi\"]\n",
    "        ndsi_mask = ((B03 - B11) / (B03 + B11)) < thresholds[\"max_ndsi\"]\n",
    "        low_rgb_mask = (B02 > min_rgb) * (B03 > min_rgb) * (B04 > min_rgb)\n",
    "        high_rgb_mask = (B02 < max_blue) * (B03 < max_green) * (B04 < max_red)\n",
    "        b11_mask = ((B11 - B03) / (B11 + B03)) < max_b11\n",
    "        b11_mask_abs = (B11 > thresholds[\"min_b11\"]) * (B11 < max_b11)\n",
    "        self.no_truck_mask = ndvi_mask * ndwi_mask * ndsi_mask * low_rgb_mask * high_rgb_mask * b11_mask * b11_mask_abs\n",
    "    \n",
    "    # Calculate a binary mask where trucks are represented as 1 and no trucks as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### min_green_ratio Float, minimum value of blue-green ratio\n",
    "    ### min_red_ratio Float, minimum value of blue-red ratio\n",
    "    def detect_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        bg_ratio = (B02 - B03) / (B02 + B03)\n",
    "        br_ratio = (B02 - B04) / (B02 + B04)\n",
    "        bg = (bg_ratio * self.no_trucks) > thresholds[\"min_green_ratio\"]\n",
    "        br = (br_ratio * self.no_trucks) > thresholds[\"min_red_ratio\"]\n",
    "        self.trucks = bg * br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-43-7a19a62a3c63>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-7a19a62a3c63>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    def write_n_observations(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class PeriodProcessor():\n",
    "    def __init__(self, start, end, bbox, bbox_id):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.bbox = bbox\n",
    "        self.bbox_id = bbox_id\n",
    "        self.cube = None\n",
    "        self.dates = None\n",
    "        self.lon_lat = None\n",
    "        self.n_observations = []\n",
    "        self.detections = []\n",
    "        self.mean_trucks = None\n",
    "        self.sum_trucks = None\n",
    "        self.sum_obs = None\n",
    "        self.mean_trucks_points = None\n",
    "    \n",
    "    def get_cube(self, dataset, bands, tile_size, spatial_res, day_bins):\n",
    "        config = CubeConfig(dataset_name = dataset,\n",
    "                            band_names = bands,\n",
    "                            tile_size = tile_size,\n",
    "                            geometry = self.bbox,\n",
    "                            time_range = [self.start, self.end],\n",
    "                            spatial_res = spatial_res)\n",
    "        cube = open_cube(config)\n",
    "        self.cube = cube\n",
    "        self.dates = cube.time.values\n",
    "        self.lon_lat = {\"lon\":cube.cube.lon.values, \"lat\":cube.cube.lat.values}\n",
    "        \n",
    "    def mask_cube(self):\n",
    "        scl = MaskSet(self.cube.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        low_prob = scl.clouds_low_probability_or_unclassified\n",
    "        cirrus = scl.cirrus\n",
    "        cloud_mask = (high_prob + med_prob + low_prob + cirrus) == 0\n",
    "        self.cube = cube.where(cloud_mask)\n",
    "    \n",
    "    # Add from acquisition methods\n",
    "    def add_n_observations(self, band):\n",
    "        obs = np.count_nonzero(np.isnan(band.values))\n",
    "        obs.dtype = np.uint16\n",
    "        self.n_observations.append(obs)\n",
    "    \n",
    "    def add_detections(self, trucks):\n",
    "        self.detections.append(trucks)\n",
    "    \n",
    "    # Summarize methods\n",
    "    def sum_trucks(self):\n",
    "        self.sum_trucks = np.array(self.detections, dtype=np.unint16).sum(axis=0)\n",
    "    \n",
    "    def sum_obs(self):\n",
    "        self.sum_obs = np.array(self.n_observations, dtype=np.uint16).sum(axis=0)\n",
    "            \n",
    "    def mean_trucks(self): \n",
    "        mean_trucks = np.round(self.sum_trucks / self.n_obs)\n",
    "        self.mean_trucks = mean_trucks.astype(np.uint16)\n",
    "                \n",
    "    def vectorize_mean_trucks(self, dirs_ts, bbox_id):\n",
    "        self.mean_trucks_points = points_from_np(self.mean_trucks, self.lon_lat)\n",
    "    \n",
    "    # Write methods\n",
    "    def write_n_observations(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_obs(dirs_ts, bbox_id, ext)\n",
    "        sum_obs_xr = create_xy_xarray(self.sum_obs, self.lon_lat)\n",
    "        sum_obs_xr.to_netcdf(sum_obs_xr, fname)\n",
    "        \n",
    "    def write_sum_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_trucks(dirs_ts, bbox_id, ext)\n",
    "        sum_xr = create_xy_xarray(self.sum_trucks, self.lon_lat)\n",
    "        sum_xr.to_netcdf(sum_xr, fname)\n",
    "    \n",
    "    def write_mean_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_mean_trucks(dirs_ts, bbox_id, ext)\n",
    "        mean_xr = create_xy_xarray(self.mean_trucks, self.lon_lat)\n",
    "        mean_xr.to_netcdf(mean_xr, fname)\n",
    "        \n",
    "    def write_mean_trucks_vec(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_mean_trucks_vec(dirs_ts, bbox_id, ext)\n",
    "        driver = GPKG() if ext == GPKG_EXT()\n",
    "        driver = GEOJSON() if ext = GEOJSON_EXT()\n",
    "        trucks_points.to_file(fname, driver = )\n",
    "    \n",
    "    def wrap_period(self, dirs_ts, bbox_id, ext_arr = NC_EXT(), ext_vec = GPKG_EXT()):\n",
    "        period.sum_obs()\n",
    "        period.sum_trucks()\n",
    "        period.mean_trucks()\n",
    "        try:\n",
    "            period.vectorize_mean_trucks()\n",
    "        except:\n",
    "            Warning(\"points_from_np failed\")\n",
    "        period.write_n_observations(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_sum_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_mean_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_mean_trucks_vec(dir_ts, bbox_id, ext_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcquisitionProcessor():\n",
    "    def __init__(self, date_np64, cube):\n",
    "        self.date_np64 = date_np64\n",
    "        self.cube = cube\n",
    "        self.band_stack = cube.sel(date_np64)\n",
    "        self.detector = TruckDetector(self.band_stack)\n",
    "    \n",
    "    # percentage Float secifies the percentage of valid observations\n",
    "    # that is needed to be considered as valid acquisition\n",
    "    def has_observations(self, minimum_valid_percentage):\n",
    "        B02 = self.band_stack.B02\n",
    "        values = B02.values.flatten()\n",
    "        n_vals = len(values)\n",
    "        n_nan = np.count_nonzero(np.isnan(values)) # nonzero = isnan\n",
    "        percent_valid = 100 - ((n_nan / n_vals) * 100)\n",
    "        return percent_valid >= minimum_valid_percentage\n",
    "    \n",
    "    def do_detection(self, thresholds):     \n",
    "        self.detector.calc_no_trucks(thresholds)\n",
    "        self.detector.detect_trucks(thresholds)\n",
    "        \n",
    "    def write_detections(self, dirs_ts, bbox_id, ext = NC_EXT()):\n",
    "        fname = fname_abs_n_trucks(dirs_ts, bbox_id, ext)\n",
    "        trucks_np = self.detector.trucks.astype(np.uint16)\n",
    "        trucks_xr = create_xy_xarray(trucks_np, {\"lon\":cube.lon.values, \"lat\":cube.lat.values})\n",
    "        trucks_xr.to_netcdf(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "____________\n",
    "## Do Processing\n",
    "Process data by __weekday__, __timestamp__ (sub period) processing grid __box__ (bbox_id).\n",
    "\n",
    "#### Make or read processing grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(files[\"proc_grid\"]):\n",
    "    try:\n",
    "        grid_gadm = gpd.read_file(files[\"proc_grid\"])\n",
    "    except:\n",
    "        raise Exception(\"Failed reading proc grid from: \" + files[\"proc_grid\"])\n",
    "else:\n",
    "    grid_gadm = make_grid(grid_spacing, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calc temporal bounds of baseline sub-periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = calc_periods(n_days_sub, baseline, target)\n",
    "periods[\"first\"].append(target[\"first\"]) # append start date of target period\n",
    "periods[\"last\"].append(target[\"last\"]) # append end date of target period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "ext_arr = NC_EXT()\n",
    "ext_vec = GPKG_EXT()\n",
    "for i in range(len(grid_gadm)):\n",
    "    bbox = grid_gadm.geometry[i].bounds\n",
    "    bbox_id = grid_gadm[BBOX_ID()][i]\n",
    "    print(\"Processing: \" + str(i))\n",
    "    print(\"bbox_id: \" + str(bbox_id))\n",
    "    try:\n",
    "        file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, buffer, dirs[\"ancil_roads\"])\n",
    "    except:\n",
    "        msg = \"Could not get OSM roads: \" + str(bbox_id)\n",
    "        Warning(msg)\n",
    "        trace.append(msg)\n",
    "        continue\n",
    "    first = periods[\"first\"]\n",
    "    last = periods[\"last\"]\n",
    "    for start, end in zip(first, last):\n",
    "        ts = first.index(start)\n",
    "        dir_ts = os.path.join(dirs[\"processed\"], \"ts_%s_%s_%s\" %(str(ts), str(first), str(last)))\n",
    "        if not os.path.exist(dir_ts) os.mkdir(dir_ts)\n",
    "        dirs_ts = make_dirs_ts(dir_ts)\n",
    "        print(\"TS: %s Start: %s End: %s\" %(str(ts), str(start), str(end)))\n",
    "        period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "        period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "        period.mask_cube()\n",
    "        n_acquisitions = 0\n",
    "        for date in period.dates:\n",
    "            date_np64 = datetime_to_np(date)\n",
    "            acquisition = AcquisitionProcessor(date_np64, period.cube)\n",
    "            if is_weekday(date, weekday) and acquisition.has_observations(minimum_valid_observations):\n",
    "                n_acquisitions += 1\n",
    "                acquisition.do_detection(thresholds)\n",
    "                acquisition.write_detections(dirs_ts, bbox_id, ext_arr)                \n",
    "                period.add_n_observations(band_stack.B02)\n",
    "                period.add_detections(detector.trucks.values)\n",
    "            else:\n",
    "                continue\n",
    "        if n_acquisitions > 0:\n",
    "            period.wrap_period(dirs_ts, bbox_id, arr_ext, ext_vec)\n",
    "        else:\n",
    "            msg = \"No acquisitions in period %s to %s. In bbox_id: %s\" %(str(start), str(end), str(bbox_id))\n",
    "            Warning(msg)\n",
    "            trace.append(msg)\n",
    "    print(\"Done with bbox_id: \" + str(bbox_id))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
