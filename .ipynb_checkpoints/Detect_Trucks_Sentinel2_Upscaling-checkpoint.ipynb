{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Trucks Sentinel-2 - Europe\n",
    "________________                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# load creds\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 1 | Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_package(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# general\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# installations\n",
    "install_package(\"OSMPythonTools\")\n",
    "install_package(\"geocube\")\n",
    "\n",
    "# OSM API\n",
    "from OSMPythonTools.overpass import overpassQueryBuilder, Overpass\n",
    "\n",
    "# xcube\n",
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "# spatial\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geocube\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "from shapely import geometry, coords\n",
    "from shapely.geometry import Polygon, Point\n",
    "#from osgeo import gdal #, gdal_array, ogr\n",
    "\n",
    "# plotting\n",
    "import matplotlib as plt\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2 | General Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # approx. 10m\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"SCL\"]\n",
    "day_bins = \"1D\" # for cube\n",
    "tile_size = [512, 512]\n",
    "minimum_valid_observations = 30 # percent\n",
    "grid_spacing = 1. # processing grid box size [degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main = os.getcwd()\n",
    "dir_not_commit = os.path.join(dir_main, \"not_commit\")\n",
    "dir_ancil = os.path.join(dir_not_commit, \"ancillary_data\")\n",
    "dirs = {\"dir\":dir_main, \"dir_not_commit\":dir_not_commit, \"ancil\":dir_ancil, \"processing\":os.path.join(dir_main, \"processing\"),\n",
    "       \"processed\":os.path.join(dir_not_commit, \"processed\"), \"ancil_roads\":os.path.join(dir_ancil, \"roads\"), \"ancil_gadm\":os.path.join(dir_ancil, \"gadm\")}\n",
    "for directory in list(dirs.values()):\n",
    "    if not os.path.exists(directory): os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\"gadm_efta\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_EU_EFTA.gpkg\"), \n",
    "         \"gadm_europe\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_europe.gpkg\"), \n",
    "         \"gadm_europe_union\":os.path.join(dirs[\"ancil_gadm\"], \"gadm0_europe_union.gpkg\"),\n",
    "         \"proc_grid\":os.path.join(dirs[\"processing\"], \"processing_grid_gadm_%s.geojson\" %(grid_spacing))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = [\"tuesday\", \"wednesday\", \"thursday\"] # process mean of these weekdays\n",
    "target = {\"first\":datetime(2020, 3, 16), \"last\":datetime(2020, 6, 10)}\n",
    "baseline = {\"first\":datetime(2017, 12, 15), \"last\":datetime(2020, 3, target[\"first\"].day - 1)}\n",
    "baseline_years = [2018, 2019] # if processing for a yearly period as in target\n",
    "n_days_sub = 91 # days per timestamp (sub-period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_key = \"highway\"\n",
    "osm_values = [\"motorway\", \"trunk\", \"primary\", \"secondary\"]\n",
    "roads_buffer = 0.00022 # degree, for motorway, the others lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 3 | Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\"min_rgb\":0.04,\n",
    "              \"min_blue\":0.1, # treat shadows\n",
    "              \"max_red\":0.15,\n",
    "              \"max_green\":0.15,\n",
    "              \"max_blue\":0.4,\n",
    "              \"max_ndvi\":0.7,\n",
    "              \"max_ndwi\":0.001,\n",
    "              \"max_ndsi\":0.0001,\n",
    "              \"min_b11\":0.05,\n",
    "              \"max_b11\":0.55,\n",
    "              \"min_green_ratio\":0.05,\n",
    "              \"min_red_ratio\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## 4 | Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPSG_4326(): return \"EPSG:4326\"\n",
    "def EPSG_3857(): return \"EPSG:3857\"\n",
    "def GEOJSON(): return \"GeoJSON\"\n",
    "def GPKG(): return \"GPKG\"\n",
    "def GEOJSON_EXT(): return \".geojson\"\n",
    "def GPKG_EXT(): return \".gpkg\"\n",
    "def NC_EXT(): return \".nc\"\n",
    "def BBOX_ID(): return \"bbox_id\"\n",
    "def ABS_N_TRUCKS(): return \"acquisitions_trucks\"\n",
    "def SUM_TRUCKS(): return \"period_sum_trucks\"\n",
    "def MEAN_N_TRUCKS(): return \"period_mean_trucks\"\n",
    "def MEAN_N_TRUCKS_VEC(): return \"period_mean_trucks_vectorized\"\n",
    "def N_OBS(): return \"n_observations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname_osm(directory, bbox_id, osm_key, ext = GPKG_EXT()): return os.path.join(directory, str(bbox_id) + \"_\" + osm_key + ext)\n",
    "def construct_fname(dirs_ts, dir_ts_key, bbox_id, ext):\n",
    "    return os.path.join(dirs_ts[dir_ts_key], dir_ts_key + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_acquisition_trucks(dirs_ts, bbox_id, date, ext = NC_EXT()): \n",
    "    return os.path.join(dirs_ts[ABS_N_TRUCKS()], str(date) + \"_\" + ABS_N_TRUCKS() + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_sum_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, SUM_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec(dirs_ts, bbox_id, ext = GPKG_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id, ext = \".txt\"): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_sum_obs(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, N_OBS(), bbox_id, ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_ts(dir_ts):\n",
    "    dir_ts_overall = os.path.join(dir_ts, \"overall\")\n",
    "    if not os.path.exists(dir_ts_overall): os.mkdir(dir_ts_overall)\n",
    "    dirs_ts = {ABS_N_TRUCKS():os.path.join(dir_ts, ABS_N_TRUCKS()), # acquisition-wise\n",
    "               SUM_TRUCKS():os.path.join(dir_ts_overall, SUM_TRUCKS()), # aggregation\n",
    "               MEAN_N_TRUCKS():os.path.join(dir_ts_overall, MEAN_N_TRUCKS()), # aggr\n",
    "               MEAN_N_TRUCKS_VEC():os.path.join(dir_ts_overall, MEAN_N_TRUCKS_VEC()), # aggr\n",
    "               N_OBS():os.path.join(dir_ts_overall, N_OBS())} # aggr\n",
    "    for direc in dirs_ts.values():\n",
    "        if not os.path.exists(direc): os.mkdir(direc)\n",
    "    return dirs_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing grid\n",
    "Create a grid covering Europe for processing chunk-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(grid_spacing, files):\n",
    "    crs = EPSG_4326()\n",
    "    gadm0_eu_efta = gpd.read_file(files[\"gadm_efta\"])\n",
    "    xmin,ymin,xmax,ymax = gadm0_eu_efta.total_bounds\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    cols = int((width) / grid_spacing)\n",
    "    rows = int((height) / grid_spacing)\n",
    "    box_width = width / cols\n",
    "    box_height = height / rows\n",
    "    boxes = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            col_right = col + 1\n",
    "            row_lower = row + 1\n",
    "            y_up = ymax-row*box_height\n",
    "            y_low = ymax-row_lower*box_height\n",
    "            x_left = xmin+col*box_width\n",
    "            x_right = xmin+col_right*box_width\n",
    "            boxes.append(Polygon([(x_left, y_up), \n",
    "                                  (x_right, y_up), \n",
    "                                  (x_right, y_low), \n",
    "                                  (x_left, y_low)]))\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":boxes})\n",
    "    grid.crs = crs\n",
    "    gadm0_europe = gpd.read_file(files[\"gadm_europe\"])\n",
    "    gadm0_europe.crs = crs\n",
    "    file_union = files[\"gadm_europe_union\"]\n",
    "    if os.path.exists(file_union):\n",
    "        gadm0_europe_clip_union = gpd.read_file(file_union)\n",
    "    else:\n",
    "        gadm0_europe[\"continent\"] = [\"EUROPE\"] * len(gadm0_europe)\n",
    "        gadm0_europe_clip = gpd.overlay(gadm0_europe, gpd.GeoDataFrame({\"a\":[1],\"geometry\":test}), how = \"intersection\")\n",
    "        gadm0_europe_clip_union = gadm0_europe_clip.dissolve(by = \"continent\")\n",
    "        gadm0_europe_clip_union.to_file(file_union, driver = GPKG())\n",
    "    # intersect with gadm for cutting boxes at coast line\n",
    "    grid_intersect = gpd.overlay(grid, gadm0_europe_clip_union, how = \"intersection\")\n",
    "    # get country attributes\n",
    "    grid_gadm = gpd.sjoin(grid_intersect, gadm0_europe, how = \"left\", op = \"intersects\")\n",
    "    grid_gadm[BBOX_ID()] = range(len(grid_gadm))\n",
    "    boxes = grid_gadm.geometry.apply(lambda geom : geom.bounds)\n",
    "    grid_gadm.geometry = [Polygon(geometry.box(b[0], b[1], b[2], b[3])) for b in boxes] # use the light bboxes\n",
    "    delete_cols = [\"GID_0_left\", \"NAME_0_left\", \"a\", \"continent\", \"index_right\"]\n",
    "    for column in delete_cols:\n",
    "        if column in grid_gadm.columns: \n",
    "            grid_gadm = grid_gadm.drop(column, 1)\n",
    "    grid_gadm = grid_gadm.rename(columns = {\"GID_0_right\":\"GID_0\", \"NAME_0_right\":\"NAME_0\"})\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"GID_0\"] != \"RUS\"] # do not include\n",
    "    grid_gadm.to_file(files[\"proc_grid\"], driver = GEOJSON())\n",
    "    return grid_gadm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lat_lon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(polygons, lat, lon, fill=np.nan):\n",
    "    transform = transform_lat_lon(lat, lon)\n",
    "    out_shape = (len(lat), len(lon))\n",
    "    raster = features.rasterize(polygons.geometry, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float)\n",
    "    return xr.DataArray(raster, coords={\"lat\":lat, \"lon\":lon}, dims=(\"lat\", \"lon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data np array\n",
    "# lat_lon dict of \"lat\" and \"lon\" holding np arrays of coordinates\n",
    "def create_xr_dataset(data, lat_lon, name):\n",
    "    data_array = xr.DataArray(data, coords = list(lat_lon.values()), dims = list(lat_lon.keys()))\n",
    "    return xr.Dataset({name:data_array})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date Datetime object to be checked\n",
    "# weekday String weekday to be checked against\n",
    "# returns Boolean if date is weekday\n",
    "def is_weekday(date_x, weekday):\n",
    "    if not isinstance(date_x, type(datetime.date)): date_x = pd.to_datetime(date_x).date()\n",
    "    weekday = weekday.lower()[0:3]\n",
    "    y, m, d = 2000, 1, 3\n",
    "    wd = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "    ref = {}\n",
    "    for i in range(len(wd)): ref[wd[i]] = datetime(y, m, d + i).date()\n",
    "    return (date_x - ref[weekday]).days % 7 == 0 # check if date is in a sequence of 7\n",
    "\n",
    "# Calculates the dates of the periods representing a timestamp each\n",
    "# n_days_sub Integer how many dates shall one timestamp (sub-period) cover\n",
    "# baseline Dict start and end dates (datetime) overall baseline period\n",
    "# target Dict start and end dates (datetime) target period\n",
    "def calc_periods(n_days_sub, baseline, target):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    base_first = baseline[fst]\n",
    "    n_days = target[fst] - base_first\n",
    "    n_sub = int(n_days.days / n_days_sub) # number of timestamps\n",
    "    start = [base_first] * n_sub\n",
    "    start = [start[i] + timedelta(n_days_sub * i) for i in range(len(start))]\n",
    "    end = [start[i+1] - timedelta(1) for i in range(len(start)-1)]\n",
    "    end.append(baseline[lst])\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    periods[\"first\"].append(target[\"first\"]) # append start date of target period\n",
    "    periods[\"last\"].append(target[\"last\"]) # append end date of target period\n",
    "    return periods\n",
    "\n",
    "# Returns periods equivalent to target period for other years\n",
    "# target Dict start and end dates (datetime) target period\n",
    "# years List of int years\n",
    "def yearly_period_from_target(target, years = [2018, 2019]):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    trgt_fst = target[fst]\n",
    "    trgt_lst = target[lst]\n",
    "    start = []\n",
    "    end = []\n",
    "    for year in years:\n",
    "        start.append(datetime(year, trgt_fst.month, trgt_fst.day))\n",
    "        end.append(datetime(year, trgt_lst.month, trgt_lst.day))\n",
    "    start.append(trgt_fst)\n",
    "    end.append(trgt_lst)\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM data\n",
    "Utils for retrieving road data from OSM through its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order bbox from W,S,E,N to S,W,N,E\n",
    "def convert_bbox_osm(bbox):\n",
    "    offset = 0.05 # add a buffer to bbox in order to be sure cube is entirely covered\n",
    "    bbox_osm = [bbox[1], bbox[0], bbox[3], bbox[2]]\n",
    "    bbox_osm[0] -= offset # min lon\n",
    "    bbox_osm[1] += offset # max lat\n",
    "    bbox_osm[2] += offset # max lon\n",
    "    bbox_osm[3] -= offset # min lat\n",
    "    return bbox_osm\n",
    "\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_value String OSM value\n",
    "# osm_key String OSM key\n",
    "# element_type List of String\n",
    "# returns GeoPandasDataFrame\n",
    "def get_osm(bbox, \n",
    "            bbox_id,\n",
    "            osm_value = \"motorway\",\n",
    "            osm_key = \"highway\", # in OSM 'highway' contains several road types: https://wiki.openstreetmap.org/wiki/Key:highway\n",
    "            element_type = [\"way\", \"relation\"]):\n",
    "    \n",
    "    bbox_osm = convert_bbox_osm(bbox)\n",
    "    quot = '\"'\n",
    "    select = quot+osm_key+quot + '=' + quot+osm_value+quot\n",
    "    select_link = select.replace(osm_value, osm_value + \"_link\") # also get road links\n",
    "    select_junction = select.replace(osm_value, osm_value + \"_junction\")\n",
    "    geoms = []\n",
    "    for selector in [select, select_link, select_junction]:  \n",
    "        try:\n",
    "            query = overpassQueryBuilder(bbox=bbox_osm, \n",
    "                                         elementType=element_type, \n",
    "                                         selector=selector, \n",
    "                                         out='body',\n",
    "                                         includeGeometry=True)\n",
    "            elements = Overpass().query(query, timeout=60).elements()\n",
    "            # create multiline of all elements\n",
    "            if len(elements) > 0:\n",
    "                for i in range(len(elements)):\n",
    "                    elem = elements[i]\n",
    "                    geoms.append(elem.geometry())\n",
    "        except:\n",
    "            Warning(\"Could not retrieve \" + select)\n",
    "    try:\n",
    "        lines = gpd.GeoDataFrame(crs = EPSG_4326(), geometry = geoms)\n",
    "        n = len(geoms)\n",
    "        lines[BBOX_ID()] = [bbox_id]*n\n",
    "        lines[\"osm_value\"] = [osm_value]*n # add road type\n",
    "        return lines\n",
    "    except:\n",
    "        Warning(\"Could not merge \" + osm_value)\n",
    "        \n",
    "# buffer Float road buffer distance [m]\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_values List of String OSM values\n",
    "# osm_key String OSM key\n",
    "# roads_buffer Float buffer width\n",
    "# dir_write\n",
    "def get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dir_write):\n",
    "    fwrite = fname_osm(dir_write, bbox_id, osm_key)\n",
    "    if not os.path.exists(fwrite):\n",
    "        roads = []\n",
    "        has_error = []\n",
    "        offset = 0.00002\n",
    "        buffer_dist = \"buffer_distance\"\n",
    "        # buffer according to road type\n",
    "        m,t,p,s,ter = \"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\"\n",
    "        buffers = {m:roads_buffer, t:roads_buffer-offset, p:roads_buffer-(2*offset), s:roads_buffer-(3*offset), ter:roads_buffer-(4*offset)}\n",
    "        osm_values_int = {m:1, t:2, p:3, s:4, ter:5}\n",
    "        for osm_value in osm_values:\n",
    "            try:\n",
    "                roads_osm = get_osm(bbox = bbox, bbox_id = bbox_id, osm_value = osm_value)\n",
    "                roads_osm[buffer_dist] = [buffers[osm_value]] * len(roads_osm)\n",
    "                roads_osm[\"osm_value_int\"] = osm_values_int[osm_value]\n",
    "                roads.append(roads_osm)\n",
    "            except:\n",
    "                has_error.append(1)\n",
    "                Warning(\"'get_osm'\" + \"failed for bbox_id \"+ str(bbox_id) + \"osm_value \" + osm_value + \"osm_key\" + osm_key)\n",
    "        if len(roads) > len(has_error):\n",
    "            roads_merge = gpd.GeoDataFrame(pd.concat(roads, ignore_index=True), crs=roads[0].crs)\n",
    "            buffered = roads_merge.buffer(distance=roads_merge[buffer_dist])\n",
    "            roads_merge.geometry = buffered\n",
    "            roads_merge.to_file(fwrite, driver = GPKG())\n",
    "    return fwrite\n",
    "\n",
    "# osm geodataframe of polygons\n",
    "# reference_raster xarray with lat and lon\n",
    "def rasterize_osm(osm, reference_raster):\n",
    "    osm_values = list(set(osm[\"osm_value\"]))\n",
    "    nan_placeholder = 100\n",
    "    road_rasters = []\n",
    "    for osm_value in osm_values:\n",
    "        osm_subset = osm[osm[\"osm_value\"] == osm_value]\n",
    "        raster = rasterize(osm_subset, reference_raster.lat, reference_raster.lon)\n",
    "        cond = np.isfinite(raster)\n",
    "        raster_osm = np.where(cond, list(osm_subset.osm_value_int)[0], nan_placeholder) # use placeholder instead of nan first\n",
    "        raster_osm = raster_osm.astype(np.float)\n",
    "        road_rasters.append(raster_osm)        \n",
    "    # merge road types in one layer\n",
    "    road_raster_np = np.array(road_rasters).min(axis=0) # now use the lowest value (highest road level) because some intersect\n",
    "    road_raster_np[road_raster_np == nan_placeholder] = 0\n",
    "    return road_raster_np # 0=no_road 1=motorway, 2=trunk, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Truck detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruckDetector detects trucks at acquisition-level\n",
    "class TruckDetector():   \n",
    "    def __init__(self, band_stack):\n",
    "        self.band_stack = band_stack\n",
    "        is_none = band_stack is None\n",
    "        self.B02 = None if is_none else band_stack.B02 \n",
    "        self.B03 = None if is_none else band_stack.B03\n",
    "        self.B04 = None if is_none else band_stack.B04\n",
    "        self.B08 = None if is_none else band_stack.B08\n",
    "        self.B11 = None if is_none else band_stack.B11\n",
    "        self.no_truck_mask = None\n",
    "        self.trucks = None\n",
    "        \n",
    "    def set_trucks(self, trucks):\n",
    "        self.trucks = trucks # for reload option\n",
    "    \n",
    "    # Calculate a binary mask where pixels that are definitely no trucks are represented as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### max_ndvi Float above this val: no trucks. For Vegetation\n",
    "    ### max_ndwi Float above this val: no trucks. For Water\n",
    "    ### max_ndsi Float above this val: no_trucks. For Snow\n",
    "    ### min_rgb Float above this val: no_trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_blue Float above this val: no_trucks\n",
    "    ### max_green Float above this val: no trucks\n",
    "    ### max_red Float above this val: no trucks\n",
    "    ### min_b11 Float below this val: no trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_b11 Float below this val: no trucks. For bright (sealed) surfaces, e.g. buildings\n",
    "    def calc_no_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        B08 = self.B08\n",
    "        B11 = self.B11\n",
    "        min_rgb = thresholds[\"min_rgb\"]\n",
    "        max_blue = thresholds[\"max_blue\"]\n",
    "        max_green = thresholds[\"max_green\"]\n",
    "        max_red = thresholds[\"max_red\"]\n",
    "        max_b11 = thresholds[\"max_b11\"]\n",
    "        ndvi_mask = ((B08 - B04) / (B08 + B04)) < thresholds[\"max_ndvi\"]\n",
    "        ndwi_mask = ((B02 - B11) / (B02 + B11)) < thresholds[\"max_ndwi\"]\n",
    "        ndsi_mask = ((B03 - B11) / (B03 + B11)) < thresholds[\"max_ndsi\"]\n",
    "        low_rgb_mask = (B02 > thresholds[\"min_blue\"]) * (B03 > min_rgb) * (B04 > min_rgb)\n",
    "        high_rgb_mask = (B02 < max_blue) * (B03 < max_green) * (B04 < max_red)\n",
    "        b11_mask = ((B11 - B03) / (B11 + B03)) < max_b11\n",
    "        b11_mask_abs = (B11 > thresholds[\"min_b11\"]) * (B11 < max_b11)\n",
    "        self.no_truck_mask = ndvi_mask * ndwi_mask * ndsi_mask * low_rgb_mask * high_rgb_mask * b11_mask * b11_mask_abs\n",
    "    \n",
    "    # Calculate a binary mask where trucks are represented as 1 and no trucks as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### min_green_ratio Float, minimum value of blue-green ratio\n",
    "    ### min_red_ratio Float, minimum value of blue-red ratio\n",
    "    def detect_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        bg_ratio = (B02 - B03) / (B02 + B03)\n",
    "        br_ratio = (B02 - B04) / (B02 + B04)\n",
    "        bg = (bg_ratio * self.no_truck_mask) > thresholds[\"min_green_ratio\"]\n",
    "        br = (br_ratio * self.no_truck_mask) > thresholds[\"min_red_ratio\"]\n",
    "        self.trucks = bg * br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 6 | Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PeriodProcessor processes a period of dates, represented in one cube\n",
    "class PeriodProcessor():\n",
    "    def __init__(self, start, end, bbox, bbox_id):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.bbox = bbox\n",
    "        self.bbox_id = bbox_id\n",
    "        self.cube = None\n",
    "        self.dates = None\n",
    "        self.lon_lat = None\n",
    "        self.n_observations = []\n",
    "        self.detections = []\n",
    "        self.mean_trucks = None\n",
    "        self.sum_trucks = None\n",
    "        self.sum_obs = None\n",
    "        self.mean_trucks_points = None\n",
    "        self.dtype = np.uint8\n",
    "    \n",
    "    def get_cube(self, dataset, bands, tile_size, spatial_res, day_bins):\n",
    "        config = CubeConfig(dataset_name = dataset,\n",
    "                            band_names = bands,\n",
    "                            tile_size = tile_size,\n",
    "                            geometry = bbox, # #self.bbox\n",
    "                            time_range = [start, end], #  # self.start, self.end\n",
    "                            spatial_res = spatial_res)\n",
    "        cube = open_cube(config)\n",
    "        self.cube = cube\n",
    "        self.dates = cube.time.values\n",
    "        self.lon_lat = {\"lon\":cube.lon.values, \"lat\":cube.lat.values}\n",
    "        \n",
    "    # Add from acquisition methods\n",
    "    def add_n_observations(self, no_clouds):\n",
    "        obs = np.where(no_clouds == 1, 1, 0)\n",
    "        self.n_observations.append(obs)\n",
    "    \n",
    "    def add_detections(self, trucks):\n",
    "        self.detections.append(trucks)\n",
    "    \n",
    "    # Temporal summary methods\n",
    "    def sum_truck_n(self):\n",
    "        self.sum_trucks = np.array(self.detections).sum(axis=0)\n",
    "    \n",
    "    def sum_observations(self):\n",
    "        self.sum_observations = np.array(self.n_observations).sum(axis=0)\n",
    "            \n",
    "    def mean_truck_n(self): \n",
    "        self.mean_trucks = np.round(self.sum_trucks / self.n_obs) # first round\n",
    "                \n",
    "    def vectorize_mean_trucks(self, crs):\n",
    "        match_value = np.array(1, dtype=self.mean_trucks.dtype)\n",
    "        self.mean_trucks_points = points_from_np(self.mean_trucks, match_value, self.lon_lat, crs=crs)\n",
    "    \n",
    "    # Write methods\n",
    "    def write_n_observations(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_obs(dirs_ts, bbox_id, ext)\n",
    "        sum_obs_xr = create_xr_dataset(self.sum_obs, self.lon_lat, os.path.basename(fname))\n",
    "        sum_obs_xr.to_netcdf(sum_obs_xr, fname)\n",
    "        \n",
    "    def write_sum_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_trucks(dirs_ts, bbox_id, ext)\n",
    "        sum_xr = create_xr_dataset(self.sum_trucks, self.lon_lat, os.path.basename(fname))\n",
    "        sum_xr.to_netcdf(sum_xr, fname)\n",
    "    \n",
    "    def write_mean_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_mean_trucks(dirs_ts, bbox_id, ext)\n",
    "        mean_xr = create_xr_dataset(self.mean_trucks, self.lon_lat, os.path.basename(fname))\n",
    "        mean_xr.to_netcdf(mean_xr, fname)\n",
    "        \n",
    "    def write_mean_trucks_vec(self, dirs_ts, bbox_id, ext):\n",
    "        n = self.mean_trucks_points is None or len(self.mean_trucks_points)\n",
    "        if n > 0:\n",
    "            fname = fname_mean_trucks_vec(dirs_ts, bbox_id, ext)\n",
    "            driver = GPKG() if ext == GPKG_EXT() else None\n",
    "            driver = GEOJSON() if ext == GEOJSON_EXT() else None\n",
    "            if driver is None: driver = GPKG()\n",
    "            self.mean_trucks_points.to_file(fname, driver = driver)\n",
    "        else:\n",
    "            # write txt as placeholder\n",
    "            fname = fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id, \".txt\")\n",
    "            with open(fname) as file:\n",
    "                file.write(\"'mean_trucks_points' has length: %s. Nothing to write\" %(str(n)))\n",
    "            \n",
    "    def wrap_period(self, dirs_ts, bbox_id, ext_arr = NC_EXT(), ext_vec = GPKG_EXT()):\n",
    "        self.sum_observations()\n",
    "        self.sum_truck_n()\n",
    "        self.mean_truck_n()\n",
    "        try:\n",
    "            self.vectorize_mean_trucks(EPSG_4326())\n",
    "        except:\n",
    "            Warning(\"points_from_np failed\")\n",
    "        self.write_n_observations(dirs_ts, bbox_id, ext_arr)\n",
    "        self.write_sum_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        self.write_mean_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        self.write_mean_trucks_vec(dir_ts, bbox_id, ext_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AcquisitionProcessor processes all valid pixels of a single acquisition in cube\n",
    "class AcquisitionProcessor():\n",
    "    def __init__(self, date_np64, cube):\n",
    "        self.date_np64 = date_np64\n",
    "        self.cube = cube\n",
    "        self.band_stack = cube.sel(time = date_np64)\n",
    "        self.detector = None\n",
    "        self.no_clouds = None\n",
    "        \n",
    "    def mask_clouds(self):\n",
    "        scl = MaskSet(self.band_stack.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        low_prob = scl.clouds_low_probability_or_unclassified\n",
    "        cirrus = scl.cirrus\n",
    "        shadows = scl.cloud_shadows\n",
    "        dark = scl.dark_area_pixels\n",
    "        self.no_clouds = (high_prob + med_prob + low_prob + cirrus + shadows + dark) == 0\n",
    "        self.band_stack = self.band_stack.where(self.no_clouds)\n",
    "    \n",
    "    # percentage Float secifies the percentage of valid observations\n",
    "    # that is needed to be considered as valid acquisition\n",
    "    def has_observations(self, minimum_valid_percentage):\n",
    "        values = self.no_clouds.values.flatten()\n",
    "        n_vals = len(values)\n",
    "        n_valid = np.count_nonzero(values)\n",
    "        percent_valid = (n_valid / n_vals) * 100\n",
    "        return percent_valid >= minimum_valid_percentage\n",
    "    \n",
    "    # osm gpd polygons\n",
    "    def mask_with_osm(self, osm):\n",
    "        osm_raster = rasterize_osm(osm, self.band_stack.B02)\n",
    "        mask_xr = create_xr_dataset(osm_raster, {\"lat\":self.cube.lat.values, \"lon\":self.cube.lon.values}, \"roadmask\")\n",
    "        self.band_stack = self.band_stack.where(mask_xr.roadmask != 0)\n",
    "                \n",
    "    def do_detection(self, thresholds):\n",
    "        self.detector = TruckDetector(self.band_stack)\n",
    "        self.detector.calc_no_trucks(thresholds)\n",
    "        self.detector.detect_trucks(thresholds)\n",
    "        \n",
    "    def write_detections(self, fname):\n",
    "        trucks_xr = create_xr_dataset(self.detector.trucks, {\"lat\":self.cube.lat.values, \"lon\":self.cube.lon.values},\n",
    "                                    os.path.basename(fname))\n",
    "        # add no_clouds layer (n observations) for possible reload\n",
    "        trucks_xr_with_no_clouds = trucks_xr.assign({\"no_clouds\":self.no_clouds})\n",
    "        trucks_xr_with_no_clouds.to_netcdf(fname)\n",
    "        \n",
    "    def read_detections(self, fname):\n",
    "        dataset = xr.open_dataset(fname)\n",
    "        self.no_clouds = dataset[\"no_clouds\"]\n",
    "        trucks = dataset[os.path.basename(fname)]\n",
    "        self.detector = TruckDetector(None)\n",
    "        self.detector.set_trucks(trucks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "____________\n",
    "## 7 | Do Processing\n",
    "Process data by __weekday__, __timestamp__ (sub period) processing grid __box__ (bbox_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make or read processing grid\n",
    "if os.path.exists(files[\"proc_grid\"]):\n",
    "    try:\n",
    "        grid_gadm = gpd.read_file(files[\"proc_grid\"])\n",
    "    except:\n",
    "        raise Exception(\"Failed reading proc grid from: \" + files[\"proc_grid\"])\n",
    "else:\n",
    "    grid_gadm = make_grid(grid_spacing, files)\n",
    "# calc temporal bounds of baseline sub-periods\n",
    "if len(baseline_years) > 0:\n",
    "    periods = yearly_period_from_target(target, baseline_years)\n",
    "else:\n",
    "    periods = calc_periods(n_days_sub, baseline, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "ext_arr = NC_EXT()\n",
    "ext_vec = GPKG_EXT()\n",
    "sep = \"-\" * 20\n",
    "for i in range(len(grid_gadm)):\n",
    "    i = list(grid_gadm[BBOX_ID()]).index(745)\n",
    "    bbox = grid_gadm.geometry[i].bounds    \n",
    "    bbox_id = grid_gadm[BBOX_ID()][i]\n",
    "    bbox_id_str = str(bbox_id)\n",
    "    first = periods[\"first\"]\n",
    "    last = periods[\"last\"]\n",
    "    print(\"%s\\nProcessing: %s\\nbbox_id: %s\" %(sep, str(i), bbox_id_str))\n",
    "    try:\n",
    "        file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        # retry\n",
    "        if not os.path.exists(file_osm): file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        osm = gpd.read_file(file_osm)\n",
    "    except:\n",
    "        msg = \"Could not get OSM roads: \" + bbox_id_str\n",
    "        Warning(msg)\n",
    "        trace.append(msg)\n",
    "        continue\n",
    "    for start, end in zip(first, last):\n",
    "        \n",
    "        test_t1 = datetime.now()\n",
    "        \n",
    "        ts = first.index(start)\n",
    "        ts_str = str(ts)\n",
    "        dir_ts = os.path.join(dirs[\"processed\"], \"ts_%s_%s_%s\" %(ts_str, str(start.date()), str(end.date())))\n",
    "        if not os.path.exists(dir_ts): os.mkdir(dir_ts)\n",
    "        dirs_ts = make_dirs_ts(dir_ts)\n",
    "        print(\"TS: %s Period Start: %s End: %s\" %(ts_str, str(start), str(end)))\n",
    "        \n",
    "        # check if yet processed\n",
    "        fname_sum_observations = fname_sum_obs(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_sum = fname_sum_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_mean = fname_mean_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_mean_vec = fname_mean_trucks_vec(dirs_ts, bbox_id, ext_vec)\n",
    "        fname_mean_vec_placeholder = fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id)\n",
    "        exist = [os.path.exists(file) for file in [fname_sum_observations, fname_sum, fname_mean, fname_mean_vec]]\n",
    "        if not exist[3]: exist[3] = os.path.exists(fname_mean_vec_placeholder) # placeholder might exist instead\n",
    "        \n",
    "        already_proc = \"because already processed\"\n",
    "        if all(exist) and not overwrite_results:\n",
    "            print(\"Skipping \" + already_proc)\n",
    "        else:\n",
    "            period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "            period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "            t1 = datetime.now() # track time cube is open to prevent timeout\n",
    "            n_acquisitions = 0\n",
    "            for period_date in period.dates:\n",
    "                acquisition = AcquisitionProcessor(period_date, period.cube)\n",
    "                date_str = str(period_date)\n",
    "                fname = fname_acquisition_trucks(dirs_ts, bbox_id, date_str, ext_arr)\n",
    "                if os.path.exists(fname):\n",
    "                    try:\n",
    "                        acquisition.read_detections(fname)\n",
    "                        period.add_n_observations(acquisition.no_clouds)\n",
    "                        period.add_detections(acquisition.detector.trucks.values)\n",
    "                        n_acquisitions += 1\n",
    "                        print(\"Read processed date: %s %s\" %(date_str, already_proc))\n",
    "                    except:\n",
    "                        Warning(\"Could not read: \" + fname)\n",
    "                else:\n",
    "                    weekday_match = any([is_weekday(period_date, w) for w in weekdays])\n",
    "                    if weekday_match:\n",
    "                        acquisition.mask_clouds()\n",
    "                        # check if enough observations to be considered\n",
    "                        if acquisition.has_observations(minimum_valid_observations): \n",
    "                            n_acquisitions += 1\n",
    "                            acquisition.mask_with_osm(osm) # mask to OSM roads\n",
    "                            acquisition.do_detection(thresholds) # truck detection\n",
    "                            print(\"Writing\")\n",
    "                            try:\n",
    "                                acquisition.write_detections(fname)\n",
    "                            except:\n",
    "                                Warning(\"Failed writing: \" + fname)\n",
    "                            period.add_n_observations(acquisition.no_clouds)\n",
    "                            period.add_detections(acquisition.detector.trucks.values)\n",
    "                            if not os.path.exists(fname): print(\"File was not written: \" + fname)\n",
    "                            print(\"Done with date: %s\" %(date_str))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "                t2 = datetime.now()\n",
    "                if (t2-t1).seconds > 3600: # if one hour is exceeded, be sure no timeout\n",
    "                    period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "            duration = datetime.now() - test_t1\n",
    "            if n_acquisitions > 0:\n",
    "                period.wrap_period(dirs_ts, bbox_id, arr_ext, ext_vec)\n",
    "            else:\n",
    "                msg = \"No acquisitions in period %s to %s. In bbox_id: %s\" %(str(start), str(end), bbox_id_str)\n",
    "                Warning(msg)\n",
    "                trace.append(msg)\n",
    "        print(\"Done with period %s of bbox_id %s\\n%s\" %(ts_str, bbox_id_str, sep))\n",
    "    print(\"Done with bbox_id: \" + bbox_id_str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
