{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Trucks Sentinel-2 - Europe\n",
    "________________                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load creds\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 1 | Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# OSM API\n",
    "from OSMPythonTools.overpass import overpassQueryBuilder, Overpass\n",
    "\n",
    "# xcube\n",
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "# spatial\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely import geometry, coords\n",
    "from shapely.geometry import Polygon\n",
    "#from osgeo import gdal #, gdal_array, ogr\n",
    "\n",
    "# plotting\n",
    "import matplotlib as plt\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2 | General Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # 10m\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"SCL\"]\n",
    "day_bins = \"1D\" # for cube\n",
    "tile_size = [512, 512]\n",
    "minimum_valid_observations = 15\n",
    "grid_spacing = 1. # processing grid box size [degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main = os.getcwd()\n",
    "dir_not_commit = os.path.join(dir_main, \"not_commit\")\n",
    "dir_ancil = os.path.join(dir_not_commit, \"ancillary_data\")\n",
    "dirs = {\"dir\":dir_main, \"dir_not_commit\":dir_not_commit, \"ancil\":dir_ancil, \"processing\":os.path.join(dir_main, \"processing\"),\n",
    "       \"processed\":os.path.join(dir_not_commit, \"processed\"), \"ancil_roads\":os.path.join(dir_ancil, \"roads\"), \"ancil_gadm\":os.path.join(dir_ancil, \"gadm\")}\n",
    "for directory in list(dirs.values()):\n",
    "    if not os.path.exists(directory): os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\"gadm_efta\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_EU_EFTA.gpkg\"), \n",
    "         \"gadm_europe\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_europe.gpkg\"), \n",
    "         \"gadm_europe_union\":os.path.join(dirs[\"ancil_gadm\"], \"gadm0_europe_union.gpkg\"),\n",
    "         \"proc_grid\":os.path.join(dirs[\"processing\"], \"processing_grid_gadm_%s.geojson\" %(grid_spacing))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = \"wednesday\" # process for this weekday\n",
    "target = {\"first\":datetime(2020, 3, 16), \"last\":datetime(2020, 6, 6)}\n",
    "baseline = {\"first\":datetime(2017, 12, 15), \"last\":datetime(2020, 3, target[\"first\"].day - 1)}\n",
    "n_days_sub = 91 # days per timestamp (sub-period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_key = \"highway\"\n",
    "osm_values = [\"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\"]\n",
    "roads_buffer = 0.00018 # degree, for motorway, the others lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 3 | Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\"min_rgb\":0.04,\n",
    "              \"max_red\":0.15,\n",
    "              \"max_green\":0.15,\n",
    "              \"max_blue\":0.4,\n",
    "              \"max_ndvi\":0.7,\n",
    "              \"max_ndwi\":0.001,\n",
    "              \"max_ndsi\":0.0001,\n",
    "              \"min_b11\":0.05,\n",
    "              \"max_b11\":0.55,\n",
    "              \"min_green_ratio\":0.05,\n",
    "              \"min_red_ratio\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## 4 | Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPSG_4326(): return \"EPSG:4326\"\n",
    "def EPSG_3857(): return \"EPSG:3857\"\n",
    "def GEOJSON(): return \"GeoJSON\"\n",
    "def GPKG(): return \"GPKG\"\n",
    "def GEOJSON_EXT(): return \".geojson\"\n",
    "def GPKG_EXT(): return \".gpkg\"\n",
    "def NC_EXT(): return \".nc\"\n",
    "def BBOX_ID(): return \"bbox_id\"\n",
    "def ABS_N_TRUCKS(): return \"abs_n_trucks\"\n",
    "def SUM_TRUCKS(): return \"sum_trucks\"\n",
    "def MEAN_N_TRUCKS(): return \"mean_n_trucks\"\n",
    "def MEAN_N_TRUCKS_VEC(): return \"mean_n_trucks_vec\"\n",
    "def N_OBS(): return \"n_observations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname_osm(directory, bbox_id, osm_key, ext = GPKG_EXT()): return os.path.join(directory, str(bbox_id) + \"_\" + osm_key + ext)\n",
    "def construct_fname(dirs_ts, dir_ts_key, bbox_id, ext):\n",
    "    return os.path.join(dirs_ts[dir_ts_key], dir_ts_key + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_abs_n_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, ABS_N_TRUCKS(), bbox_id, ext)\n",
    "def fname_sum_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, SUM_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec(dirs_ts, bbox_id, ext = GPKG_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id, ext = \".txt\"): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_sum_obs(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, N_OBS(), bbox_id, ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_ts(dir_ts):\n",
    "    dir_ts_overall = os.path.join(dir_ts, \"overall\")\n",
    "    if not os.path.exists(dir_ts_overall): os.mkdir(dir_ts_overall)\n",
    "    dirs_ts = {ABS_N_TRUCKS():os.path.join(dir_ts, ABS_N_TRUCKS()), # acquisition-wise\n",
    "               SUM_TRUCKS():os.path.join(dir_ts_overall, SUM_TRUCKS()), # aggregation\n",
    "               MEAN_N_TRUCKS():os.path.join(dir_ts_overall, MEAN_N_TRUCKS()), # aggr\n",
    "               MEAN_N_TRUCKS_VEC():os.path.join(dir_ts_overall, MEAN_N_TRUCKS_VEC()), # aggr\n",
    "               N_OBS():os.path.join(dir_ts_overall, N_OBS())} # aggr\n",
    "    for direc in dirs_ts.values():\n",
    "        if not os.path.exists(direc): os.mkdir(direc)\n",
    "    return dirs_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing grid\n",
    "Create a grid covering Europe for processing chunk-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(grid_spacing, files):\n",
    "    crs = EPSG_4326()\n",
    "    gadm0_eu_efta = gpd.read_file(files[\"gadm_efta\"])\n",
    "    xmin,ymin,xmax,ymax = gadm0_eu_efta.total_bounds\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    cols = int((width) / grid_spacing)\n",
    "    rows = int((height) / grid_spacing)\n",
    "    box_width = width / cols\n",
    "    box_height = height / rows\n",
    "    boxes = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            col_right = col + 1\n",
    "            row_lower = row + 1\n",
    "            y_up = ymax-row*box_height\n",
    "            y_low = ymax-row_lower*box_height\n",
    "            x_left = xmin+col*box_width\n",
    "            x_right = xmin+col_right*box_width\n",
    "            boxes.append(Polygon([(x_left, y_up), \n",
    "                                  (x_right, y_up), \n",
    "                                  (x_right, y_low), \n",
    "                                  (x_left, y_low)]))\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":boxes})\n",
    "    grid.crs = crs\n",
    "    gadm0_europe = gpd.read_file(files[\"gadm_europe\"])\n",
    "    gadm0_europe.crs = crs\n",
    "    file_union = files[\"gadm_europe_union\"]\n",
    "    if os.path.exists(file_union):\n",
    "        gadm0_europe_clip_union = gpd.read_file(file_union)\n",
    "    else:\n",
    "        gadm0_europe[\"continent\"] = [\"EUROPE\"] * len(gadm0_europe)\n",
    "        gadm0_europe_clip = gpd.overlay(gadm0_europe, gpd.GeoDataFrame({\"a\":[1],\"geometry\":test}), how = \"intersection\")\n",
    "        gadm0_europe_clip_union = gadm0_europe_clip.dissolve(by = \"continent\")\n",
    "        gadm0_europe_clip_union.to_file(file_union, driver = GPKG())\n",
    "    # intersect with gadm for cutting boxes at coast line\n",
    "    grid_intersect = gpd.overlay(grid, gadm0_europe_clip_union, how = \"intersection\")\n",
    "    # get country attributes\n",
    "    grid_gadm = gpd.sjoin(grid_intersect, gadm0_europe, how = \"left\", op = \"intersects\")\n",
    "    grid_gadm[BBOX_ID()] = range(len(grid_gadm))\n",
    "    boxes = grid_gadm.geometry.apply(lambda geom : geom.bounds)\n",
    "    grid_gadm.geometry = [Polygon(geometry.box(b[0], b[1], b[2], b[3])) for b in boxes] # use the light bboxes\n",
    "    delete_cols = [\"GID_0_left\", \"NAME_0_left\", \"a\", \"continent\", \"index_right\"]\n",
    "    for column in delete_cols:\n",
    "        if column in grid_gadm.columns: \n",
    "            grid_gadm = grid_gadm.drop(column, 1)\n",
    "    grid_gadm = grid_gadm.rename(columns = {\"GID_0_right\":\"GID_0\", \"NAME_0_right\":\"NAME_0\"})\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"GID_0\"] != \"RUS\"] # do not include\n",
    "    grid_gadm.to_file(files[\"proc_grid\"], driver = GEOJSON())\n",
    "    return grid_gadm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data np array\n",
    "# lat_lon dict of \"lat\" and \"lon\" holding np arrays of coordinates\n",
    "def create_xy_xarray(data, lat_lon):\n",
    "    return xr.DataArray(data, coords = list(lat_lon.values()), dims = list(lat_lon.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date Datetime object to be checked\n",
    "# weekday String weekday to be checked against\n",
    "# returns Boolean if date is weekday\n",
    "def is_weekday(date_x, weekday):\n",
    "    if not isinstance(date_x, type(datetime.date)): date_x = pd.to_datetime(date_x).date()\n",
    "    weekday = weekday.lower()[0:3]\n",
    "    y, m, d = 2000, 1, 3\n",
    "    wd = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "    ref = {}\n",
    "    for i in range(len(wd)): ref[wd[i]] = datetime(y, m, d + i).date()\n",
    "    return (date_x - ref[weekday]).days % 7 == 0 # check if date is in a sequence of 7\n",
    "\n",
    "# Calculates the dates of the periods representing a timestamp each\n",
    "# n_days_sub Integer how many dates shall one timestamp (sub-period) cover\n",
    "# baseline Dict start and end dates overall baseline period\n",
    "# target Dict start and end dates target period\n",
    "def calc_periods(n_days_sub, baseline, target):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    base_first = baseline[fst]\n",
    "    n_days = target[fst] - base_first\n",
    "    n_sub = int(n_days.days / n_days_sub) # number of timestamps\n",
    "    start = [base_first] * n_sub\n",
    "    start = [start[i] + timedelta(n_days_sub * i) for i in range(len(start))]\n",
    "    end = [start[i+1] - timedelta(1) for i in range(len(start)-1)]\n",
    "    end.append(baseline[lst])\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM data\n",
    "Utils for retrieving road data from OSM through its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order bbox from W,S,E,N to S,W,N,E\n",
    "def convert_bbox_osm(bbox):\n",
    "    return [bbox[1], bbox[0], bbox[3], bbox[2]]\n",
    "\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_value String OSM value\n",
    "# osm_key String OSM key\n",
    "# element_type List of String\n",
    "# returns GeoPandasDataFrame\n",
    "def get_osm(bbox, \n",
    "            bbox_id,\n",
    "            osm_value = \"motorway\",\n",
    "            osm_key = \"highway\", # in OSM 'highway' contains several road types: https://wiki.openstreetmap.org/wiki/Key:highway\n",
    "            element_type = [\"way\", \"relation\"]):\n",
    "    \n",
    "    bbox_osm = convert_bbox_osm(bbox)\n",
    "    quot = '\"'\n",
    "    select = quot+osm_key+quot + '=' + quot+osm_value+quot\n",
    "    select_link = select.replace(osm_value, osm_value + \"_link\") # also get road links\n",
    "    select_junction = select.replace(osm_value, osm_value + \"_junction\")\n",
    "    geoms = []\n",
    "    for selector in [select, select_link, select_junction]:  \n",
    "        try:\n",
    "            query = overpassQueryBuilder(bbox=bbox_osm, \n",
    "                                         elementType=element_type, \n",
    "                                         selector=selector, \n",
    "                                         out='body',\n",
    "                                         includeGeometry=True)\n",
    "            elements = Overpass().query(query, timeout=60).elements()\n",
    "            # create multiline of all elements\n",
    "            if len(elements) > 0:\n",
    "                for i in range(len(elements)):\n",
    "                    elem = elements[i]\n",
    "                    geoms.append(elem.geometry())\n",
    "        except:\n",
    "            Warning(\"Could not retrieve \" + select)\n",
    "    try:\n",
    "        lines = gpd.GeoDataFrame(crs = EPSG_4326(), geometry = geoms)\n",
    "        n = len(geoms)\n",
    "        lines[BBOX_ID()] = [bbox_id]*n\n",
    "        lines[\"osm_value\"] = [osm_value]*n # add road type\n",
    "        return lines\n",
    "    except:\n",
    "        Warning(\"Could not merge \" + osm_value)\n",
    "        \n",
    "# buffer Float road buffer distance [m]\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_values List of String OSM values\n",
    "# osm_key String OSM key\n",
    "# buffer Float buffer width\n",
    "# dir_write\n",
    "def get_roads(bbox, bbox_id, osm_values, osm_key, buffer, dir_write):\n",
    "    fwrite = fname_osm(dir_write, bbox_id, osm_key)\n",
    "    if not os.path.exists(fwrite):\n",
    "        roads = []\n",
    "        has_error = []\n",
    "        offset = 0.00002\n",
    "        buffer_dist = \"buffer_distance\"\n",
    "        # buffer according to road type\n",
    "        buffers = {\"motorway\":buffer, \"primary\":buffer-offset, \"secondary\":buffer-(2*offset), \"tertiary\":buffer-(3*offset)}\n",
    "        for osm_value in osm_values:\n",
    "            try:\n",
    "                roads_osm = get_osm(bbox = bbox, bbox_id = bbox_id, osm_value = osm_value)\n",
    "                roads_osm[buffer_dist] = [buffers[osm_value]] * len(roads_osm)\n",
    "                roads.append(roads_osm)\n",
    "            except:\n",
    "                has_error.append(1)\n",
    "                Warning(\"'get_osm'\" + \"failed for bbox_id \"+ str(bbox_id) + \"osm_value \" + osm_value + \"osm_key\" + osm_key)\n",
    "        if len(roads) > len(has_error):\n",
    "            roads_merge = gpd.GeoDataFrame(pd.concat(roads, ignore_index=True), crs=roads[0].crs)\n",
    "            buffered = roads_merge.buffer(distance=roads_merge[buffer_dist])\n",
    "            roads_merge.geometry = buffered\n",
    "            roads_merge.to_file(fwrite, driver = GPKG())\n",
    "    return fwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Truck detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruckDetector detects trucks at acquisition-level\n",
    "class TruckDetector():\n",
    "    def __init__(self, band_stack):\n",
    "        self.band_stack = band_stack\n",
    "        self.B02 = band_stack.B02\n",
    "        self.B03 = band_stack.B03\n",
    "        self.B04 = band_stack.B04\n",
    "        self.B08 = band_stack.B08\n",
    "        self.B11 = band_stack.B11\n",
    "        self.no_truck_mask = None # xarray\n",
    "        self.trucks = None # xarray\n",
    "    \n",
    "    # Calculate a binary mask where pixels that are definitely no trucks are represented as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### max_ndvi Float above this val: no trucks. For Vegetation\n",
    "    ### max_ndwi Float above this val: no trucks. For Water\n",
    "    ### max_ndsi Float above this val: no_trucks. For Snow\n",
    "    ### min_rgb Float above this val: no_trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_blue Float above this val: no_trucks\n",
    "    ### max_green Float above this val: no trucks\n",
    "    ### max_red Float above this val: no trucks\n",
    "    ### min_b11 Float below this val: no trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_b11 Float below this val: no trucks. For bright (sealed) surfaces, e.g. buildings\n",
    "    def calc_no_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        B08 = self.B08\n",
    "        B11 = self.B11\n",
    "        min_rgb = thresholds[\"min_rgb\"]\n",
    "        max_blue = thresholds[\"max_blue\"]\n",
    "        max_green = thresholds[\"max_green\"]\n",
    "        max_red = thresholds[\"max_red\"]\n",
    "        max_b11 = thresholds[\"max_b11\"]\n",
    "        ndvi_mask = ((B08 - B04) / (B08 + B04)) < thresholds[\"max_ndvi\"]\n",
    "        ndwi_mask = ((B02 - B11) / (B02 + B11)) < thresholds[\"max_ndwi\"]\n",
    "        ndsi_mask = ((B03 - B11) / (B03 + B11)) < thresholds[\"max_ndsi\"]\n",
    "        low_rgb_mask = (B02 > min_rgb) * (B03 > min_rgb) * (B04 > min_rgb)\n",
    "        high_rgb_mask = (B02 < max_blue) * (B03 < max_green) * (B04 < max_red)\n",
    "        b11_mask = ((B11 - B03) / (B11 + B03)) < max_b11\n",
    "        b11_mask_abs = (B11 > thresholds[\"min_b11\"]) * (B11 < max_b11)\n",
    "        self.no_truck_mask = ndvi_mask * ndwi_mask * ndsi_mask * low_rgb_mask * high_rgb_mask * b11_mask * b11_mask_abs\n",
    "    \n",
    "    # Calculate a binary mask where trucks are represented as 1 and no trucks as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### min_green_ratio Float, minimum value of blue-green ratio\n",
    "    ### min_red_ratio Float, minimum value of blue-red ratio\n",
    "    def detect_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        bg_ratio = (B02 - B03) / (B02 + B03)\n",
    "        br_ratio = (B02 - B04) / (B02 + B04)\n",
    "        bg = (bg_ratio * self.no_truck_mask) > thresholds[\"min_green_ratio\"]\n",
    "        br = (br_ratio * self.no_truck_mask) > thresholds[\"min_red_ratio\"]\n",
    "        self.trucks = bg * br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 6 | Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PeriodProcessor processes a period of dates, represented in one cube\n",
    "class PeriodProcessor():\n",
    "    def __init__(self, start, end, bbox, bbox_id):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.bbox = bbox\n",
    "        self.bbox_id = bbox_id\n",
    "        self.cube = None\n",
    "        self.dates = None\n",
    "        self.lon_lat = None\n",
    "        self.n_observations = []\n",
    "        self.detections = []\n",
    "        self.mean_trucks = None\n",
    "        self.sum_trucks = None\n",
    "        self.sum_obs = None\n",
    "        self.mean_trucks_points = None\n",
    "    \n",
    "    def get_cube(self, dataset, bands, tile_size, spatial_res, day_bins):\n",
    "        config = CubeConfig(dataset_name = dataset,\n",
    "                            band_names = bands,\n",
    "                            tile_size = tile_size,\n",
    "                            geometry = bbox, # #self.bbox\n",
    "                            time_range = [start, end], #  # self.start, self.end\n",
    "                            spatial_res = spatial_res)\n",
    "        cube = open_cube(config)\n",
    "        self.cube = cube\n",
    "        self.dates = cube.time.values\n",
    "        self.lon_lat = {\"lon\":cube.lon.values, \"lat\":cube.lat.values}\n",
    "        \n",
    "    # Add from acquisition methods\n",
    "    def add_n_observations(self, band):\n",
    "        obs = np.count_nonzero(np.isnan(band.values))\n",
    "        obs.dtype = np.uint16\n",
    "        self.n_observations.append(obs)\n",
    "    \n",
    "    def add_detections(self, trucks):\n",
    "        self.detections.append(trucks)\n",
    "    \n",
    "    # Summarize methods\n",
    "    def sum_trucks(self):\n",
    "        self.sum_trucks = np.array(self.detections, dtype=np.unint16).sum(axis=0)\n",
    "    \n",
    "    def sum_obs(self):\n",
    "        self.sum_obs = np.array(self.n_observations, dtype=np.uint16).sum(axis=0)\n",
    "            \n",
    "    def mean_trucks(self): \n",
    "        mean_trucks = np.round(self.sum_trucks / self.n_obs)\n",
    "        self.mean_trucks = mean_trucks.astype(np.uint16)\n",
    "                \n",
    "    def vectorize_mean_trucks(self, crs):\n",
    "        match_value = np.array(1, dtype=np.unint16)\n",
    "        self.mean_trucks_points = points_from_np(self.mean_trucks, match_value, self.lon_lat, crs=crs)\n",
    "    \n",
    "    # Write methods\n",
    "    def write_n_observations(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_obs(dirs_ts, bbox_id, ext)\n",
    "        sum_obs_xr = create_xy_xarray(self.sum_obs, self.lon_lat)\n",
    "        sum_obs_xr.to_netcdf(sum_obs_xr, fname)\n",
    "        \n",
    "    def write_sum_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_sum_trucks(dirs_ts, bbox_id, ext)\n",
    "        sum_xr = create_xy_xarray(self.sum_trucks, self.lon_lat)\n",
    "        sum_xr.to_netcdf(sum_xr, fname)\n",
    "    \n",
    "    def write_mean_trucks(self, dirs_ts, bbox_id, ext):\n",
    "        fname = fname_mean_trucks(dirs_ts, bbox_id, ext)\n",
    "        mean_xr = create_xy_xarray(self.mean_trucks, self.lon_lat)\n",
    "        mean_xr.to_netcdf(mean_xr, fname)\n",
    "        \n",
    "    def write_mean_trucks_vec(self, dirs_ts, bbox_id, ext):\n",
    "        n = self.mean_trucks_points is None or len(self.mean_trucks_points)\n",
    "        if n > 0:\n",
    "            fname = fname_mean_trucks_vec(dirs_ts, bbox_id, ext)\n",
    "            driver = GPKG() if ext == GPKG_EXT() else None\n",
    "            driver = GEOJSON() if ext == GEOJSON_EXT() else None\n",
    "            if driver is None: driver = GPKG()\n",
    "            self.mean_trucks_points.to_file(fname, driver = driver)\n",
    "        else:\n",
    "            # write txt as placeholder\n",
    "            fname = fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id, \".txt\")\n",
    "            with open(fname) as file:\n",
    "                file.write(\"'mean_trucks_points' has length: %s. Nothing to write\" %(str(n)))\n",
    "            \n",
    "    def wrap_period(self, dirs_ts, bbox_id, ext_arr = NC_EXT(), ext_vec = GPKG_EXT()):\n",
    "        period.sum_obs()\n",
    "        period.sum_trucks()\n",
    "        period.mean_trucks()\n",
    "        try:\n",
    "            period.vectorize_mean_trucks(EPSG_4326())\n",
    "        except:\n",
    "            Warning(\"points_from_np failed\")\n",
    "        period.write_n_observations(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_sum_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_mean_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        period.write_mean_trucks_vec(dir_ts, bbox_id, ext_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AcquisitionProcessor processes all valid pixels of a single acquisition in cube\n",
    "class AcquisitionProcessor():\n",
    "    def __init__(self, date_np64, cube):\n",
    "        self.date_np64 = date_np64\n",
    "        self.cube = cube\n",
    "        self.band_stack = cube.sel(time = date_np64)\n",
    "        self.detector = TruckDetector(self.band_stack)\n",
    "    \n",
    "    # percentage Float secifies the percentage of valid observations\n",
    "    # that is needed to be considered as valid acquisition\n",
    "    def has_observations(self, minimum_valid_percentage):\n",
    "        B02 = self.band_stack.B02\n",
    "        values = B02.values.flatten()\n",
    "        n_vals = len(values)\n",
    "        n_nan = np.count_nonzero(np.isnan(values)) # nonzero = isnan\n",
    "        percent_valid = 100 - ((n_nan / n_vals) * 100)\n",
    "        return percent_valid >= minimum_valid_percentage\n",
    "    \n",
    "    def mask_clouds(self):\n",
    "        scl = MaskSet(self.band_stack.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        low_prob = scl.clouds_low_probability_or_unclassified\n",
    "        cirrus = scl.cirrus\n",
    "        no_clouds = (high_prob + med_prob + low_prob + cirrus) == 0\n",
    "        self.band_stack = self.band_stack.where(no_clouds)\n",
    "        \n",
    "    def do_detection(self, thresholds):     \n",
    "        self.detector.calc_no_trucks(thresholds)\n",
    "        self.detector.detect_trucks(thresholds)\n",
    "        \n",
    "    def write_detections(self, fname):\n",
    "        trucks_np = self.detector.trucks.astype(np.uint16)\n",
    "        trucks_xr = create_xy_xarray(trucks_np, {\"lon\":cube.lon.values, \"lat\":cube.lat.values})\n",
    "        trucks_xr.to_netcdf(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "____________\n",
    "## 7 Do Processing\n",
    "Process data by __weekday__, __timestamp__ (sub period) processing grid __box__ (bbox_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make or read processing grid\n",
    "if os.path.exists(files[\"proc_grid\"]):\n",
    "    try:\n",
    "        grid_gadm = gpd.read_file(files[\"proc_grid\"])\n",
    "    except:\n",
    "        raise Exception(\"Failed reading proc grid from: \" + files[\"proc_grid\"])\n",
    "else:\n",
    "    grid_gadm = make_grid(grid_spacing, files)\n",
    "    \n",
    "# calc temporal bounds of baseline sub-periods\n",
    "periods = calc_periods(n_days_sub, baseline, target)\n",
    "periods[\"first\"].append(target[\"first\"]) # append start date of target period\n",
    "periods[\"last\"].append(target[\"last\"]) # append end date of target period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "ext_arr = NC_EXT()\n",
    "ext_vec = GPKG_EXT()\n",
    "sep = \"-\" * 10\n",
    "for i in range(len(grid_gadm)):\n",
    "    # i = list(grid_gadm[BBOX_ID()]).index(821) # for testing\n",
    "    bbox = grid_gadm.geometry[i].bounds\n",
    "    bbox_id = grid_gadm[BBOX_ID()][i]\n",
    "    bbox_id_str = str(bbox_id)\n",
    "    print(\"Processing: \" + str(i))\n",
    "    print(\"bbox_id: \" + bbox_id_str)\n",
    "    try:\n",
    "        file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        # retry\n",
    "        if not os.path.exists(file_osm): file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        osm = gpd.read_file(file_osm)\n",
    "    except:\n",
    "        msg = \"Could not get OSM roads: \" + bbox_id_str\n",
    "        Warning(msg)\n",
    "        trace.append(msg)\n",
    "        continue\n",
    "    first = periods[\"first\"]\n",
    "    last = periods[\"last\"]\n",
    "    for start, end in zip(first, last):\n",
    "        ts = first.index(start)\n",
    "        ts_str = str(ts)\n",
    "        dir_ts = os.path.join(dirs[\"processed\"], \"ts_%s_%s_%s\" %(ts_str, str(start.date()), str(end.date())))\n",
    "        if not os.path.exists(dir_ts): os.mkdir(dir_ts)\n",
    "        dirs_ts = make_dirs_ts(dir_ts)\n",
    "        print(\"TS: %s Start: %s End: %s\" %(ts_str, str(start), str(end)))\n",
    "        \n",
    "        # check if yet processed\n",
    "        fname_sum_obs = fname_sum_obs(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_sum = fname_sum_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_mean = fname_mean_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "        fname_mean_vec = fname_mean_trucks_vec(dirs_ts, bbox_id, ext_vec)\n",
    "        fname_mean_vec_placeholder = fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id)\n",
    "        exist = [os.path.exists(file) for file in [fname_sum_obs, fname_sum, fname_mean, fname_mean_vec]]\n",
    "        if not exist[3]: exist[3] = os.path.exists(fname_mean_vec_placeholder) # placeholder might exist instead\n",
    "        \n",
    "        already_proc = \"because already processed\"\n",
    "        if all(exist) && not overwrite_results:\n",
    "            print(\"Skipping \" + already_proc)\n",
    "        else:\n",
    "            period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "            period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "            t1 = datetime.now() # track time cube is open to prevent timeout\n",
    "            n_acquisitions = 0\n",
    "            for period_date in period.dates:\n",
    "                acquisition = AcquisitionProcessor(period_date, period.cube)\n",
    "                if is_weekday(period_date, weekday) and acquisition.has_observations(minimum_valid_observations):\n",
    "                    date_str = str(period_date)\n",
    "                    fname = fname_abs_n_trucks(dirs_ts, bbox_id, ext_arr)\n",
    "                    if os.path.exists(fname):\n",
    "                        print(\"Skipping date: %s %s\" %(date_str, already_proc))\n",
    "                    else:\n",
    "                        n_acquisitions += 1\n",
    "                        acquisition.mask_clouds()\n",
    "                        acquisition.mask_with_osm()\n",
    "                        acquisition.do_detection(thresholds)\n",
    "                        acquisition.write_detections(fname)                \n",
    "                        period.add_n_observations(band_stack.B02)\n",
    "                        period.add_detections(detector.trucks.values)\n",
    "                        print(\"Done with date: %s\" %(date_str))\n",
    "                else:\n",
    "                    continue\n",
    "                t2 = datetime.now()\n",
    "                if (t2-t1).seconds > 3600: # if one hour is exceeded, be sure no timeout\n",
    "                    period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)                    \n",
    "            if n_acquisitions > 0:\n",
    "                period.wrap_period(dirs_ts, bbox_id, arr_ext, ext_vec)\n",
    "            else:\n",
    "                msg = \"No acquisitions in period %s to %s. In bbox_id: %s\" %(str(start), str(end), bbox_id_str)\n",
    "                Warning(msg)\n",
    "                trace.append(msg)\n",
    "        print(\"Done with period %s of bbox_id %s\\n%s\" %(ts_str, bbox_id_str, sep))\n",
    "    print(\"Done with bbox_id: \" + bbox_id_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
