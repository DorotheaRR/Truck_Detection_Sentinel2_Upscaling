{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Trucks Sentinel-2 - Europe\n",
    "________________                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load creds\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 1 | Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# installations\n",
    "def install_package(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "install_package(\"OSMPythonTools\")\n",
    "install_package(\"geocube\")\n",
    "\n",
    "# OSM API\n",
    "from OSMPythonTools.overpass import overpassQueryBuilder, Overpass\n",
    "\n",
    "# xcube\n",
    "from xcube_sh.cube import open_cube\n",
    "from xcube_sh.config import CubeConfig\n",
    "from xcube.core.maskset import MaskSet\n",
    "\n",
    "# spatial\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geocube\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "from shapely import geometry, coords\n",
    "from shapely.geometry import Polygon, Point\n",
    "#from osgeo import gdal #, gdal_array, ogr\n",
    "\n",
    "# plotting\n",
    "import matplotlib as plt\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2 | General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_results = True\n",
    "country = \"Germany\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"S2L2A\"\n",
    "spatial_res = 0.00009 # approx. 10m\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"SCL\"]\n",
    "day_bins = \"1D\" # for cube\n",
    "tile_size = [512, 512]\n",
    "minimum_valid_observations = 25 # percent\n",
    "grid_spacing = 1. # processing grid box size [degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main = os.getcwd()\n",
    "dir_not_commit = os.path.join(dir_main, \"not_commit\")\n",
    "dir_ancil = os.path.join(dir_not_commit, \"ancillary_data\")\n",
    "dirs = {\"dir\":dir_main, \"dir_not_commit\":dir_not_commit, \"ancil\":dir_ancil, \"processing\":os.path.join(dir_main, \"processing\"),\n",
    "       \"processed\":os.path.join(dir_not_commit, \"processed\"), \"ancil_roads\":os.path.join(dir_ancil, \"roads\"), \"ancil_gadm\":os.path.join(dir_ancil, \"gadm\")}\n",
    "for directory in list(dirs.values()):\n",
    "    if not os.path.exists(directory): os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\"gadm_efta\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_EU_EFTA.gpkg\"), \n",
    "         \"gadm_europe\":os.path.join(dirs[\"ancil_gadm\"], \"gadm36_0_europe.gpkg\"), \n",
    "         \"gadm_europe_union\":os.path.join(dirs[\"ancil_gadm\"], \"gadm0_europe_union.gpkg\"),\n",
    "         \"proc_grid\":os.path.join(dirs[\"processing\"], \"processing_grid_gadm_%s.geojson\" %(grid_spacing))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = [\"tuesday\", \"wednesday\", \"thursday\"] \n",
    "target = {\"first\":datetime(2020, 3, 16), \"last\":datetime(2020, 6, 10)}\n",
    "baseline = {\"first\":datetime(2019, 3, 16), \"last\":datetime(2020, 3, target[\"first\"].day - 1)}\n",
    "n_days_sub = 91 # days per timestamp (sub-period)\n",
    "baseline_years = [] # if processing for a yearly period as in target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_key = \"highway\"\n",
    "osm_values = [\"motorway\", \"trunk\", \"primary\"]\n",
    "roads_buffer = 0.00020 # degree, for motorway, the others lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 3 | Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\"min_rgb\":0.05,\n",
    "              \"min_blue\":0.06,\n",
    "              \"max_red\":0.15,\n",
    "              \"max_green\":0.2,\n",
    "              \"max_blue\":0.3,\n",
    "              \"max_ndvi\":0.5,\n",
    "              \"max_ndwi\":0.0001,\n",
    "              \"max_ndsi\":0.0001,\n",
    "              \"min_blue_green_ratio\":0.01,\n",
    "              \"min_blue_red_ratio\":0.04}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## 4 | Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPSG_4326(): return \"EPSG:4326\"\n",
    "def EPSG_3857(): return \"EPSG:3857\"\n",
    "def GEOJSON(): return \"GeoJSON\"\n",
    "def GPKG(): return \"GPKG\"\n",
    "def GEOJSON_EXT(): return \".geojson\"\n",
    "def GPKG_EXT(): return \".gpkg\"\n",
    "def NC_EXT(): return \".nc\"\n",
    "def BBOX_ID(): return \"bbox_id\"\n",
    "def ABS_N_TRUCKS(): return \"acquisitions_trucks\"\n",
    "def SUM_TRUCKS(): return \"period_sum_trucks\"\n",
    "def SUM_TRUCKS_VEC(): return \"period_sum_trucks_vectorized\"\n",
    "def SUM_TRUCKS_PHR(): return \"period_sum_trucks_placeholder\"\n",
    "def MEAN_N_TRUCKS(): return \"period_mean_trucks\"\n",
    "def MEAN_N_TRUCKS_VEC(): return \"period_mean_trucks_vectorized\"\n",
    "def MEAN_N_TRUCKS_PHR(): return \"period_mean_trucks_placeholder\"\n",
    "def TRUCKS_VEC(): return \"trucks_points\"\n",
    "def TRUCKS_VEC_PHR(): return \"trucks_points_placeholder\"\n",
    "def N_OBS(): return \"n_observations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname_osm(directory, bbox_id, osm_key, ext = GPKG_EXT()): return os.path.join(directory, str(bbox_id) + \"_\" + osm_key + ext)\n",
    "def construct_fname(dirs_ts, dir_ts_key, bbox_id, ext):\n",
    "    return os.path.join(dirs_ts[dir_ts_key], dir_ts_key + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_acquisition_trucks(dirs_ts, bbox_id, date, ext = NC_EXT()): \n",
    "    return os.path.join(dirs_ts[ABS_N_TRUCKS()], str(date) + \"_\" + ABS_N_TRUCKS() + \"_\" + BBOX_ID() + str(bbox_id) + ext)\n",
    "def fname_sum_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, SUM_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec(dirs_ts, bbox_id, ext = GPKG_EXT()): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_sum_trucks_vec(dirs_ts, bbox_id, ext = GPKG_EXT()): \n",
    "    return construct_fname(dirs_ts, SUM_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_mean_trucks_vec_placeholder(dirs_ts, bbox_id, ext = \".txt\"): \n",
    "    return construct_fname(dirs_ts, MEAN_N_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_sum_trucks_vec_placeholder(dirs_ts, bbox_id, ext = \".txt\"): \n",
    "    return construct_fname(dirs_ts, SUM_TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_sum_obs(dirs_ts, bbox_id, ext = NC_EXT()): \n",
    "    return construct_fname(dirs_ts, N_OBS(), bbox_id, ext)\n",
    "def fname_trucks_vec(dirs_ts, bbox_id, ext):\n",
    "    return construct_fname(dirs_ts, TRUCKS_VEC(), bbox_id, ext)\n",
    "def fname_trucks_vec_placeholder(dirs_ts, bbox_id, ext = \".txt\"):\n",
    "    return construct_fname(dirs_ts, TRUCKS_VEC(), bbox_id, ext)\n",
    "\n",
    "def sub_period_fnames(dirs_ts, bbox_id, ext_arr, ext_vec):\n",
    "    files = {N_OBS():fname_sum_obs(dirs_ts, bbox_id, ext_vec),\n",
    "             TRUCKS_VEC():fname_trucks_vec(dirs_ts, bbox_id, ext_vec),\n",
    "             TRUCKS_VEC_PHR():fname_trucks_vec_placeholder(dirs_ts, bbox_id)}\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_ts(dir_ts):\n",
    "    dir_ts_overall = os.path.join(dir_ts, \"overall\")\n",
    "    if not os.path.exists(dir_ts_overall): os.mkdir(dir_ts_overall)\n",
    "    dirs_ts = {ABS_N_TRUCKS():os.path.join(dir_ts, ABS_N_TRUCKS()),\n",
    "               N_OBS():os.path.join(dir_ts_overall, N_OBS()),\n",
    "               TRUCKS_VEC():os.path.join(dir_ts_overall, TRUCKS_VEC())}\n",
    "    for direc in dirs_ts.values():\n",
    "        if not os.path.exists(direc): os.mkdir(direc)\n",
    "    return dirs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_recurs(directory, validation):\n",
    "    if validation == \"YES_DELETE_THAT\":\n",
    "        shutil.rmtree(directory)\n",
    "    else:\n",
    "        raise Exception(\"Validation wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing grid\n",
    "Create a grid covering Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(grid_spacing, files):\n",
    "    crs = EPSG_4326()\n",
    "    gadm0_eu_efta = gpd.read_file(files[\"gadm_efta\"])\n",
    "    xmin,ymin,xmax,ymax = gadm0_eu_efta.total_bounds\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    cols = int((width) / grid_spacing)\n",
    "    rows = int((height) / grid_spacing)\n",
    "    box_width = width / cols\n",
    "    box_height = height / rows\n",
    "    boxes = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            col_right = col + 1\n",
    "            row_lower = row + 1\n",
    "            y_up = ymax-row*box_height\n",
    "            y_low = ymax-row_lower*box_height\n",
    "            x_left = xmin+col*box_width\n",
    "            x_right = xmin+col_right*box_width\n",
    "            boxes.append(Polygon([(x_left, y_up), \n",
    "                                  (x_right, y_up), \n",
    "                                  (x_right, y_low), \n",
    "                                  (x_left, y_low)]))\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":boxes})\n",
    "    grid.crs = crs\n",
    "    gadm0_europe = gpd.read_file(files[\"gadm_europe\"])\n",
    "    gadm0_europe.crs = crs\n",
    "    file_union = files[\"gadm_europe_union\"]\n",
    "    if os.path.exists(file_union):\n",
    "        gadm0_europe_clip_union = gpd.read_file(file_union)\n",
    "    else:\n",
    "        gadm0_europe[\"continent\"] = [\"EUROPE\"] * len(gadm0_europe)\n",
    "        gadm0_europe_clip = gpd.overlay(gadm0_europe, gpd.GeoDataFrame({\"a\":[1],\"geometry\":test}), how = \"intersection\")\n",
    "        gadm0_europe_clip_union = gadm0_europe_clip.dissolve(by = \"continent\")\n",
    "        gadm0_europe_clip_union.to_file(file_union, driver = GPKG())\n",
    "    # intersect with gadm for cutting boxes at coast line\n",
    "    grid_intersect = gpd.overlay(grid, gadm0_europe_clip_union, how = \"intersection\")\n",
    "    # get country attributes\n",
    "    grid_gadm = gpd.sjoin(grid_intersect, gadm0_europe, how = \"left\", op = \"intersects\")\n",
    "    grid_gadm[BBOX_ID()] = range(len(grid_gadm))\n",
    "    boxes = grid_gadm.geometry.apply(lambda geom : geom.bounds)\n",
    "    grid_gadm.geometry = [Polygon(geometry.box(b[0], b[1], b[2], b[3])) for b in boxes] # use the light bboxes\n",
    "    delete_cols = [\"GID_0_left\", \"NAME_0_left\", \"a\", \"continent\", \"index_right\"]\n",
    "    for column in delete_cols:\n",
    "        if column in grid_gadm.columns: \n",
    "            grid_gadm = grid_gadm.drop(column, 1)\n",
    "    grid_gadm = grid_gadm.rename(columns = {\"GID_0_right\":\"GID_0\", \"NAME_0_right\":\"NAME_0\"})\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"GID_0\"] != \"RUS\"] # do not include\n",
    "    grid_gadm.to_file(files[\"proc_grid\"], driver = GEOJSON())\n",
    "    return grid_gadm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lat_lon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(polygons, lat, lon, fill=np.nan):\n",
    "    transform = transform_lat_lon(lat, lon)\n",
    "    out_shape = (len(lat), len(lon))\n",
    "    raster = features.rasterize(polygons.geometry, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float)\n",
    "    return xr.DataArray(raster, coords={\"lat\":lat, \"lon\":lon}, dims=(\"lat\", \"lon\"))\n",
    "\n",
    "def vec_driver_from_ext(ext):\n",
    "    return {GPKG_EXT():GPKG(), GEOJSON_EXT():GEOJSON()}[ext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data np array\n",
    "# lat_lon dict of \"lat\" and \"lon\" holding np arrays of coordinates\n",
    "def create_xr_dataset(data, lat_lon, name):\n",
    "    lat, lon = \"lat\", \"lon\"\n",
    "    data_array = xr.DataArray(data, coords={lat:lat_lon[lat], lon:lat_lon[lon]}, dims=(lat, lon))\n",
    "    return xr.Dataset({name:data_array})\n",
    "\n",
    "# extracts coordinates at value in np array and returns points as GeoDataFrame\n",
    "# data 2d np array\n",
    "# match_value Float value in data where point coordinates are extracted\n",
    "# lon_lat dict of:\n",
    "### \"lon\": np array longitude values\"\n",
    "### \"lat\": np array latitude values\"\n",
    "# crs String EPSG:XXXX\n",
    "def points_from_np(data, match_value, lon_lat, crs):\n",
    "    indices = np.argwhere(data == match_value)\n",
    "    if len(indices) > 0:\n",
    "        lat_indices = indices[:,[0]]\n",
    "        lon_indices = indices[:,[1]]\n",
    "        lat_coords = lon_lat[\"lat\"][lat_indices]\n",
    "        lon_coords = lon_lat[\"lon\"][lon_indices]\n",
    "        points = gpd.GeoDataFrame(geometry = gpd.points_from_xy(lon_coords, lat_coords))\n",
    "        points.crs = crs\n",
    "        return points\n",
    "    \n",
    "def raster_to_points(raster, lon_lat, field_name, crs):\n",
    "    points_list = []\n",
    "    match_values = np.unique(raster[(raster != 0) * ~np.isnan(raster)]) # by pixel value\n",
    "    for x in match_values:\n",
    "        points = points_from_np(raster, x, lon_lat, crs=crs)\n",
    "        points[field_name] = [x] * len(points)\n",
    "        points_list.append(points)\n",
    "    return gpd.GeoDataFrame(pd.concat(points_list, ignore_index=True))\n",
    "\n",
    "# take xarray and ensure each value with 1 in data has no neighbor with 1 in an extended 3x3 block. Extended means: horizontally and vertically\n",
    "# it is also checked for the second-next pixel\n",
    "# Method checks only surrounding of values equal 1\n",
    "# arr xarray DataArray with values and lat lon\n",
    "def filter_spatial_3x3_extended(arr):\n",
    "    values = arr.values\n",
    "    lon = arr.lon\n",
    "    lat = arr.lat\n",
    "    valid = np.where(arr == 1)\n",
    "    for y,x in zip(valid[0], valid[1]):\n",
    "        y_above = y - 1\n",
    "        y_above_next = y - 2\n",
    "        x_left = x - 1\n",
    "        x_right = x + 1\n",
    "        x_left_next = x - 2\n",
    "        space_left = x_left >= 0\n",
    "        space_right = x_right >= 0 and x_right < len(lon)\n",
    "        space_above = y_above >= 0\n",
    "        val_left_above = values[y_above, x_left] if space_left and space_above else 0\n",
    "        val_right_above = values[y_above, x_right] if space_right and space_above else 0\n",
    "        val_left = values[y, x_left] if space_left else 0\n",
    "        val_above = values[y_above, x] if space_above else 0\n",
    "        val_left_next = values[y, x_left_next] if x_left_next >= 0 else 0\n",
    "        val_above_next = values[y_above_next, x] if y_above_next >= 0 else 0\n",
    "        # if any of the values left, above and left above has 1 set current value 0\n",
    "        if (val_left_above + val_right_above + val_left + val_above + val_left_next + val_above_next) >= 1:\n",
    "            values[y,x] = 0\n",
    "    arr.values = values\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date Datetime object to be checked\n",
    "# weekday String weekday to be checked against\n",
    "# returns Boolean if date is weekday\n",
    "def is_weekday(date_x, weekday):\n",
    "    if not isinstance(date_x, type(datetime.date)): date_x = pd.to_datetime(date_x).date()\n",
    "    weekday = weekday.lower()[0:3]\n",
    "    y, m, d = 2000, 1, 3\n",
    "    wd = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "    ref = {}\n",
    "    for i in range(len(wd)): ref[wd[i]] = datetime(y, m, d + i).date()\n",
    "    return (date_x - ref[weekday]).days % 7 == 0 # check if date is in a sequence of 7\n",
    "\n",
    "# Calculates the dates of the periods representing a timestamp each\n",
    "# n_days_sub Integer how many dates shall one timestamp (sub-period) cover\n",
    "# baseline Dict start and end dates (datetime) overall baseline period\n",
    "# target Dict start and end dates (datetime) target period\n",
    "def calc_periods(n_days_sub, baseline, target):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    base_first = baseline[fst]\n",
    "    n_days = target[fst] - base_first\n",
    "    n_sub = int(n_days.days / n_days_sub) # number of timestamps\n",
    "    start = [base_first] * n_sub\n",
    "    start = [start[i] + timedelta(n_days_sub * i) for i in range(len(start))]\n",
    "    end = [start[i+1] - timedelta(1) for i in range(len(start)-1)]\n",
    "    end.append(baseline[lst])\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    periods[\"first\"].append(target[\"first\"]) # append start date of target period\n",
    "    periods[\"last\"].append(target[\"last\"]) # append end date of target period\n",
    "    return periods\n",
    "\n",
    "# Returns periods equivalent to target period for other years\n",
    "# target Dict start and end dates (datetime) target period\n",
    "# years List of int years\n",
    "def yearly_period_from_target(target, years = [2018, 2019]):\n",
    "    fst, lst = \"first\", \"last\"\n",
    "    trgt_fst = target[fst]\n",
    "    trgt_lst = target[lst]\n",
    "    start = []\n",
    "    end = []\n",
    "    for year in years:\n",
    "        start.append(datetime(year, trgt_fst.month, trgt_fst.day))\n",
    "        end.append(datetime(year, trgt_lst.month, trgt_lst.day))\n",
    "    start.append(trgt_fst)\n",
    "    end.append(trgt_lst)\n",
    "    periods = {\"ts\":range(len(start)), fst:start, lst:end}\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSM data\n",
    "Utils for retrieving road data from OSM through its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order bbox from W,S,E,N to S,W,N,E\n",
    "def convert_bbox_osm(bbox):\n",
    "    offset = 0.05 # add a buffer to bbox in order to be sure cube is entirely covered\n",
    "    bbox_osm = [bbox[1], bbox[0], bbox[3], bbox[2]]\n",
    "    bbox_osm[0] -= offset # min lat\n",
    "    bbox_osm[1] -= offset # min lon\n",
    "    bbox_osm[2] += offset # max lat\n",
    "    bbox_osm[3] += offset # max lon\n",
    "    return bbox_osm\n",
    "\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_value String OSM value\n",
    "# osm_key String OSM key\n",
    "# element_type List of String\n",
    "# returns GeoPandasDataFrame\n",
    "def get_osm(bbox, \n",
    "            bbox_id,\n",
    "            osm_value = \"motorway\",\n",
    "            osm_key = \"highway\", # in OSM 'highway' contains several road types: https://wiki.openstreetmap.org/wiki/Key:highway\n",
    "            element_type = [\"way\", \"relation\"]):\n",
    "    \n",
    "    bbox_osm = convert_bbox_osm(bbox)\n",
    "    quot = '\"'\n",
    "    select = quot+osm_key+quot + '=' + quot+osm_value+quot\n",
    "    select_link = select.replace(osm_value, osm_value + \"_link\") # also get road links\n",
    "    select_junction = select.replace(osm_value, osm_value + \"_junction\")\n",
    "    geoms = []\n",
    "    for selector in [select, select_link, select_junction]:  \n",
    "        try:\n",
    "            query = overpassQueryBuilder(bbox=bbox_osm, \n",
    "                                         elementType=element_type, \n",
    "                                         selector=selector, \n",
    "                                         out='body',\n",
    "                                         includeGeometry=True)\n",
    "            elements = Overpass().query(query, timeout=60).elements()\n",
    "            # create multiline of all elements\n",
    "            if len(elements) > 0:\n",
    "                for i in range(len(elements)):\n",
    "                    elem = elements[i]\n",
    "                    geoms.append(elem.geometry())\n",
    "        except:\n",
    "            Warning(\"Could not retrieve \" + select)\n",
    "    try:\n",
    "        lines = gpd.GeoDataFrame(crs = EPSG_4326(), geometry = geoms)\n",
    "        n = len(geoms)\n",
    "        lines[BBOX_ID()] = [bbox_id]*n\n",
    "        lines[\"osm_value\"] = [osm_value]*n # add road type\n",
    "        return lines\n",
    "    except:\n",
    "        Warning(\"Could not merge \" + osm_value)\n",
    "        \n",
    "# buffer Float road buffer distance [m]\n",
    "# bbox List of four coords\n",
    "# bbox_id Integer processing id of bbox\n",
    "# osm_values List of String OSM values\n",
    "# osm_key String OSM key\n",
    "# roads_buffer Float buffer width\n",
    "# dir_write\n",
    "def get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dir_write):\n",
    "    fwrite = fname_osm(dir_write, bbox_id, osm_key)\n",
    "    if not os.path.exists(fwrite):\n",
    "        roads = []\n",
    "        has_error = []\n",
    "        offset = 0.00002\n",
    "        buffer_dist = \"buffer_distance\"\n",
    "        # buffer according to road type\n",
    "        m,t,p,s,ter = \"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\"\n",
    "        buffers = {m:roads_buffer, t:roads_buffer-offset, p:roads_buffer-(2*offset), s:roads_buffer-(3*offset), ter:roads_buffer-(4*offset)}\n",
    "        osm_values_int = {m:1, t:2, p:3, s:4, ter:5}\n",
    "        for osm_value in osm_values:\n",
    "            try:\n",
    "                roads_osm = get_osm(bbox = bbox, bbox_id = bbox_id, osm_value = osm_value)\n",
    "                roads_osm[buffer_dist] = [buffers[osm_value]] * len(roads_osm)\n",
    "                roads_osm[\"osm_value_int\"] = osm_values_int[osm_value]\n",
    "                roads.append(roads_osm)\n",
    "            except:\n",
    "                has_error.append(1)\n",
    "                Warning(\"'get_osm'\" + \"failed for bbox_id \"+ str(bbox_id) + \"osm_value \" + osm_value + \"osm_key\" + osm_key)\n",
    "        if len(roads) > len(has_error):\n",
    "            roads_merge = gpd.GeoDataFrame(pd.concat(roads, ignore_index=True), crs=roads[0].crs)\n",
    "            buffered = roads_merge.buffer(distance=roads_merge[buffer_dist])\n",
    "            roads_merge.geometry = buffered\n",
    "            roads_merge.to_file(fwrite, driver = GPKG())\n",
    "    return fwrite\n",
    "\n",
    "# osm geodataframe of polygons\n",
    "# reference_raster xarray with lat and lon\n",
    "def rasterize_osm(osm, reference_raster):\n",
    "    osm_values = list(set(osm[\"osm_value\"]))\n",
    "    nan_placeholder = 100\n",
    "    road_rasters = []\n",
    "    for osm_value in osm_values:\n",
    "        osm_subset = osm[osm[\"osm_value\"] == osm_value]\n",
    "        raster = rasterize(osm_subset, reference_raster.lat, reference_raster.lon)\n",
    "        cond = np.isfinite(raster)\n",
    "        raster_osm = np.where(cond, list(osm_subset.osm_value_int)[0], nan_placeholder) # use placeholder instead of nan first\n",
    "        raster_osm = raster_osm.astype(np.float)\n",
    "        road_rasters.append(raster_osm)        \n",
    "    # merge road types in one layer\n",
    "    road_raster_np = np.array(road_rasters).min(axis=0) # now use the lowest value (highest road level) because some intersect\n",
    "    road_raster_np[road_raster_np == nan_placeholder] = 0\n",
    "    return road_raster_np # 0=no_road 1=motorway, 2=trunk, ...\n",
    "\n",
    "def osm_values_to_name(values):\n",
    "    mapping = {1:\"Motorway\", 2:\"Trunk\", 3:\"Primary\", 4:\"Secondary\", 5:\"Tertiary\"}\n",
    "    return [mapping[value] for value in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 5 | Truck detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruckDetector detects trucks at acquisition-level\n",
    "class TruckDetector():   \n",
    "    def __init__(self, band_stack):\n",
    "        self.band_stack = band_stack\n",
    "        is_none = band_stack is None\n",
    "        self.B02 = None if is_none else band_stack.B02 \n",
    "        self.B03 = None if is_none else band_stack.B03\n",
    "        self.B04 = None if is_none else band_stack.B04\n",
    "        self.B08 = None if is_none else band_stack.B08\n",
    "        self.B11 = None if is_none else band_stack.B11\n",
    "        self.no_truck_mask = None\n",
    "        self.trucks = None\n",
    "        \n",
    "    def set_trucks(self, trucks):\n",
    "        self.trucks = trucks # for reload option\n",
    "    \n",
    "    # Calculate a binary mask where pixels that are definitely no trucks are represented as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### max_ndvi Float above this val: no trucks. For Vegetation\n",
    "    ### max_ndwi Float above this val: no trucks. For Water\n",
    "    ### max_ndsi Float above this val: no_trucks. For Snow\n",
    "    ### min_rgb Float above this val: no_trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_blue Float above this val: no_trucks\n",
    "    ### max_green Float above this val: no trucks\n",
    "    ### max_red Float above this val: no trucks\n",
    "    ### min_b11 Float below this val: no trucks. For dark surfaces, e.g. shadows\n",
    "    ### max_b11 Float below this val: no trucks. For bright (sealed) surfaces, e.g. buildings\n",
    "    def calc_no_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        B08 = self.B08\n",
    "        B11 = self.B11\n",
    "        min_rgb = thresholds[\"min_rgb\"]\n",
    "        max_blue = thresholds[\"max_blue\"]\n",
    "        max_green = thresholds[\"max_green\"]\n",
    "        max_red = thresholds[\"max_red\"]\n",
    "        ndvi_mask = ((B08 - B04) / (B08 + B04)) < thresholds[\"max_ndvi\"]\n",
    "        ndwi_mask = ((B02 - B11) / (B02 + B11)) < thresholds[\"max_ndwi\"]\n",
    "        ndsi_mask = ((B03 - B11) / (B03 + B11)) < thresholds[\"max_ndsi\"]\n",
    "        low_rgb_mask = (B02 > thresholds[\"min_blue\"]) * (B03 > min_rgb) * (B04 > min_rgb)\n",
    "        high_rgb_mask = (B02 < max_blue) * (B03 < max_green) * (B04 < max_red)\n",
    "        self.no_truck_mask = ndvi_mask * ndwi_mask * ndsi_mask * low_rgb_mask * high_rgb_mask\n",
    "    \n",
    "    # Calculate a binary mask where trucks are represented as 1 and no trucks as 0.\n",
    "    # thresholds Dict with at least:\n",
    "    ### min_green_ratio Float, minimum value of blue-green ratio\n",
    "    ### min_red_ratio Float, minimum value of blue-red ratio\n",
    "    def detect_trucks(self, thresholds):\n",
    "        B02 = self.B02\n",
    "        B03 = self.B03\n",
    "        B04 = self.B04\n",
    "        bg_ratio = (B02 - B03) / (B02 + B03)\n",
    "        br_ratio = (B02 - B04) / (B02 + B04)\n",
    "        bg = (bg_ratio * self.no_truck_mask) > thresholds[\"min_blue_green_ratio\"]\n",
    "        br = (br_ratio * self.no_truck_mask) > thresholds[\"min_blue_red_ratio\"]\n",
    "        self.trucks = bg * br\n",
    "        \n",
    "    def filter_trucks(self):\n",
    "        self.trucks = filter_spatial_3x3_extended(self.trucks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## 6 | Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PeriodProcessor processes a period of dates, represented in one cube\n",
    "class PeriodProcessor():\n",
    "    def __init__(self, start, end, bbox, bbox_id):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.bbox = bbox\n",
    "        self.bbox_id = bbox_id\n",
    "        self.cube = None\n",
    "        self.dates = None\n",
    "        self.lon_lat = None\n",
    "        self.n_observations = []\n",
    "        self.detections = []\n",
    "        self.mean_trucks = None\n",
    "        self.sum_trucks = None\n",
    "        self.sum_obs = None\n",
    "        self.trucks_points_result = None\n",
    "        self.osm_mask = None\n",
    "        self.files = []\n",
    "    \n",
    "    def get_cube(self, dataset, bands, tile_size, spatial_res, day_bins):\n",
    "        config = CubeConfig(dataset_name = dataset,\n",
    "                            band_names = bands,\n",
    "                            tile_size = tile_size,\n",
    "                            geometry = bbox, # #self.bbox\n",
    "                            time_range = [start, end], #  # self.start, self.end\n",
    "                            spatial_res = spatial_res)\n",
    "        cube = open_cube(config)\n",
    "        self.cube = cube\n",
    "        self.dates = cube.time.values\n",
    "        self.lon_lat = {\"lon\":cube.lon.values, \"lat\":cube.lat.values}\n",
    "    \n",
    "    def calc_osm_mask(self, osm):\n",
    "        osm_raster = rasterize_osm(osm, self.cube.B02.sel(time = self.dates[0]))\n",
    "        self.osm_mask = create_xr_dataset(osm_raster, self.lon_lat, \"roadmask\")\n",
    "    \n",
    "    # Add from acquisition methods\n",
    "    def add_n_observations(self, no_clouds):\n",
    "        obs = np.where(no_clouds == 1, 1, 0)\n",
    "        self.n_observations.append(obs)\n",
    "    \n",
    "    def add_detections(self, trucks):\n",
    "        self.detections.append(trucks)\n",
    "    \n",
    "    # Temporal summary methods\n",
    "    def sum_truck_n(self):\n",
    "        self.sum_trucks = np.array(self.detections).sum(axis=0)\n",
    "    \n",
    "    def sum_observations(self):\n",
    "        self.sum_obs = np.array(self.n_observations).sum(axis=0)\n",
    "    \n",
    "    def mask_sum_obs_to_trucks(self):\n",
    "        self.sum_obs[self.sum_trucks == 0] = 0\n",
    "    \n",
    "    def mean_truck_n(self):\n",
    "        self.mean_trucks = np.divide(self.sum_trucks, self.sum_obs)\n",
    "        self.mean_trucks[np.isnan(self.mean_trucks)] = 0\n",
    "    \n",
    "    def mask_osm_to_trucks(self, osm_mask):\n",
    "        self.osm_mask = self.osm_mask.roadmask.values\n",
    "        self.osm_mask[self.sum_trucks == 0] = 0\n",
    "            \n",
    "    # Write methods\n",
    "    def write_n_observations(self, fname):\n",
    "        sum_obs_xr = create_xr_dataset(self.sum_obs, self.lon_lat, os.path.basename(fname))\n",
    "        sum_obs_xr.to_netcdf(fname)\n",
    "        \n",
    "    def write_sum_trucks(self, fname):\n",
    "        sum_xr = create_xr_dataset(period.sum_trucks, period.lon_lat, os.path.basename(fname))\n",
    "        sum_xr.to_netcdf(fname)\n",
    "    \n",
    "    def write_mean_trucks(self, fname):\n",
    "        mean_xr = create_xr_dataset(self.mean_trucks, self.lon_lat, os.path.basename(fname))\n",
    "        mean_xr.to_netcdf(fname)\n",
    "        \n",
    "    def write_trucks_vec(self, points, fname, fname_placeholder):\n",
    "        got_points = points is not None and len(points) > 0\n",
    "        if got_points:\n",
    "            points.to_file(fname, driver = vec_driver_from_ext(\".\" + fname.split(\".\")[1]))\n",
    "        else:\n",
    "            # write txt as placeholder\n",
    "            with open(fname_placeholder, \"w\") as file:\n",
    "                file.write(\"%s has length: %s. Nothing to write\" %(os.path.basename(fname_placeholder), str(len(points))))\n",
    "        \n",
    "    def wrap_period(self, fnames):\n",
    "        crs = EPSG_4326()\n",
    "        ind_right = \"index_right\"\n",
    "        self.sum_observations()\n",
    "        self.write_n_observations(fnames[N_OBS()])\n",
    "        self.sum_truck_n()\n",
    "        self.mean_truck_n()\n",
    "        self.mask_osm_to_trucks(self.osm_mask) # mask to trucks\n",
    "        self.mask_sum_obs_to_trucks() # mask to trucks\n",
    "        # merge n observations, n trucks and mean trucks into single points layer\n",
    "        n_obs_points = raster_to_points(self.sum_obs, self.lon_lat, \"sum_observations\", crs)\n",
    "        self.sum_trucks_points = raster_to_points(self.sum_trucks, self.lon_lat, \"sum_trucks\", crs)\n",
    "        sum_trucks_points = raster_to_points(self.sum_trucks, self.lon_lat, \"sum_trucks_sub_period\", crs)\n",
    "        sum_trucks_obs = gpd.sjoin(sum_trucks_points, n_obs_points, how=\"inner\", op=\"intersects\")\n",
    "        sum_trucks_obs = sum_trucks_obs.drop([ind_right], axis=1)\n",
    "        osm_points = raster_to_points(self.osm_mask, self.lon_lat, \"osm_value\", crs)\n",
    "        osm_points[\"osm_name\"] = osm_values_to_name(osm_points[\"osm_value\"])\n",
    "        sum_trucks_obs_osm = gpd.sjoin(sum_trucks_obs, osm_points, how=\"inner\", op=\"intersects\")\n",
    "        sum_trucks_obs_osm = sum_trucks_obs_osm.drop([ind_right], axis=1)\n",
    "        mean_trucks_points = raster_to_points(self.mean_trucks, self.lon_lat, \"mean_trucks\", crs)\n",
    "        self.trucks_points_result = gpd.sjoin(sum_trucks_obs_osm, mean_trucks_points, how=\"inner\", op=\"intersects\")\n",
    "        self.trucks_points_result = self.trucks_points_result.drop([ind_right], axis=1)\n",
    "        self.write_trucks_vec(self.trucks_points_result, fnames[TRUCKS_VEC()], fnames[TRUCKS_VEC_PHR()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AcquisitionProcessor processes all valid pixels of a single acquisition in cube\n",
    "class AcquisitionProcessor():\n",
    "    def __init__(self, date_np64, cube):\n",
    "        self.date_np64 = date_np64\n",
    "        self.cube = cube\n",
    "        self.band_stack = cube.sel(time = date_np64)\n",
    "        self.detector = None\n",
    "        self.no_clouds = None\n",
    "        self.osm_mask = None\n",
    "        \n",
    "    def mask_clouds(self):\n",
    "        scl = MaskSet(acquisition.band_stack.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        cirrus = scl.cirrus\n",
    "        shadows = scl.cloud_shadows\n",
    "        no_data = scl.no_data\n",
    "        acquisition.no_clouds = (high_prob + med_prob + cirrus + shadows + no_data) == 0\n",
    "        acquisition.band_stack = acquisition.band_stack.where(acquisition.no_clouds)\n",
    "        \n",
    "        scl = MaskSet(self.band_stack.SCL)\n",
    "        high_prob = scl.clouds_high_probability\n",
    "        med_prob = scl.clouds_medium_probability\n",
    "        cirrus = scl.cirrus\n",
    "        shadows = scl.cloud_shadows\n",
    "        no_data = scl.no_data\n",
    "        self.no_clouds = (high_prob + med_prob + cirrus + shadows + no_data) == 0\n",
    "        self.band_stack = self.band_stack.where(self.no_clouds)\n",
    "    \n",
    "    # percentage Float secifies the percentage of valid observations\n",
    "    # that is needed to be considered as valid acquisition\n",
    "    def has_observations(self, minimum_valid_percentage):\n",
    "        values = self.no_clouds.values.ravel()\n",
    "        n_vals = len(values)\n",
    "        n_valid = np.count_nonzero(values)\n",
    "        percent_valid = (n_valid / n_vals) * 100\n",
    "        return percent_valid >= minimum_valid_percentage\n",
    "    \n",
    "    # osm gpd polygons\n",
    "    def mask_with_osm(self, osm_mask):\n",
    "        self.band_stack = self.band_stack.where(osm_mask.roadmask != 0)\n",
    "                \n",
    "    def do_detection(self, thresholds):\n",
    "        self.detector = TruckDetector(self.band_stack)\n",
    "        self.detector.calc_no_trucks(thresholds)\n",
    "        self.detector.detect_trucks(thresholds)\n",
    "        self.detector.filter_trucks()\n",
    "        \n",
    "    def write_detections(self, fname):\n",
    "        trucks_xr = create_xr_dataset(self.detector.trucks, {\"lat\":self.cube.lat.values, \"lon\":self.cube.lon.values},\n",
    "                                    os.path.basename(fname))\n",
    "        # add no_clouds layer (n observations) for possible reload\n",
    "        trucks_xr_with_no_clouds = trucks_xr.assign({\"no_clouds\":self.no_clouds})\n",
    "        trucks_xr_with_no_clouds.to_netcdf(fname)\n",
    "        \n",
    "    def read_detections(self, fname):\n",
    "        dataset = xr.open_dataset(fname)\n",
    "        self.no_clouds = dataset[\"no_clouds\"]\n",
    "        trucks = dataset[os.path.basename(fname)]\n",
    "        self.detector = TruckDetector(None)\n",
    "        self.detector.set_trucks(trucks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "____________\n",
    "## 7 | Do Processing\n",
    "Process data by __weekday__, __timestamp__ (sub period), processing grid __box__ (bbox_id)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing: 29\n",
      "bbox_id: 745\n",
      "TS: 0 Period Start: 2019-03-16 00:00:00 End: 2019-06-14 00:00:00\n",
      "Getting cube\n"
     ]
    }
   ],
   "source": [
    "# make or read processing grid\n",
    "if os.path.exists(files[\"proc_grid\"]):\n",
    "    try:\n",
    "        grid_gadm = gpd.read_file(files[\"proc_grid\"])\n",
    "    except:\n",
    "        raise Exception(\"Failed reading proc grid from: \" + files[\"proc_grid\"])\n",
    "else:\n",
    "    grid_gadm = make_grid(grid_spacing, files)\n",
    "if country is not None:\n",
    "    grid_gadm = grid_gadm[grid_gadm[\"NAME_0\"]==country]\n",
    "# calc temporal bounds of baseline sub-periods\n",
    "if len(baseline_years) > 0:\n",
    "    periods = yearly_period_from_target(target, baseline_years)\n",
    "else:\n",
    "    periods = calc_periods(n_days_sub, baseline, target)\n",
    "trace = []\n",
    "ext_arr = NC_EXT()\n",
    "ext_vec = GPKG_EXT()\n",
    "sep = \"-\" * 50\n",
    "n_boxes = len(grid_gadm)\n",
    "n_boxes = 2 # testing\n",
    "for i in range(n_boxes):\n",
    "    i = list(grid_gadm[BBOX_ID()]).index(745) # for testing\n",
    "    bbox = list(grid_gadm.geometry)[i].bounds    \n",
    "    bbox_id = list(grid_gadm[BBOX_ID()])[i]\n",
    "    bbox_id_str = str(bbox_id)\n",
    "    first, last = periods[\"first\"], periods[\"last\"]\n",
    "    print(\"%s\\nProcessing: %s\\nbbox_id: %s\" %(sep, str(i), bbox_id_str))\n",
    "    try:\n",
    "        file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        # retry\n",
    "        if not os.path.exists(file_osm): file_osm = get_roads(bbox, bbox_id, osm_values, osm_key, roads_buffer, dirs[\"ancil_roads\"])\n",
    "        osm = gpd.read_file(file_osm)\n",
    "        # subset according to desired osm road levels (file might have been written with other values)\n",
    "        value_not_desired = [v not in osm_values for v in list(osm[\"osm_value\"])]\n",
    "        osm = osm.drop(osm[value_not_desired].index)\n",
    "    except:\n",
    "        msg = \"Could not get OSM roads: \" + bbox_id_str\n",
    "        Warning(msg)\n",
    "        trace.append(msg)\n",
    "        #continue\n",
    "    for start, end in zip(first, last):\n",
    "        tracker_start = datetime.now()\n",
    "        ts = first.index(start)\n",
    "        ts_str = str(ts)\n",
    "        dir_ts = os.path.join(dirs[\"processed\"], \"ts_%s_%s_%s\" %(ts_str, str(start.date()), str(end.date())))\n",
    "        if not os.path.exists(dir_ts): os.mkdir(dir_ts)\n",
    "        dirs_ts = make_dirs_ts(dir_ts)\n",
    "        print(\"TS: %s Period Start: %s End: %s\" %(ts_str, str(start), str(end)))\n",
    "        \n",
    "        # check if yet processed\n",
    "        fnames = sub_period_fnames(dirs_ts, bbox_id, ext_arr, ext_vec)\n",
    "        exists = {}\n",
    "        for key, f in fnames.items():\n",
    "            exists[key] = os.path.exists(f) if not f[-3:]==\"txt\" else True       \n",
    "        if not exists[TRUCKS_VEC()]: exists[TRUCKS_VEC()] = os.path.exists(fnames[TRUCKS_VEC_PHR()]) # placeholder might exist instead\n",
    "        already_proc = \"because already processed\"\n",
    "        if all(exists.values()) and not overwrite_results:\n",
    "            print(\"Skipping \" + already_proc)\n",
    "        else:\n",
    "            print(\"Getting cube\")\n",
    "            period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "            period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "            t1 = datetime.now() # track time cube is open to prevent timeout\n",
    "            period.calc_osm_mask(osm)\n",
    "            for period_date in period.dates:\n",
    "                if (datetime.now()-t1).seconds > 1800: # if 30 minutes exceeded get cube again to prevent timeout\n",
    "                    print(\"Getting cube again to prevent timeout\")\n",
    "                    period = PeriodProcessor(start, end, bbox, bbox_id)\n",
    "                    period.get_cube(dataset, bands, tile_size, spatial_res, day_bins)\n",
    "                    t1 = datetime.now()\n",
    "                    period.calc_osm_mask(osm)\n",
    "                acquisition = AcquisitionProcessor(period_date, period.cube)\n",
    "                date_str = str(period_date).replace(\":\",\"_\")[0:-10]\n",
    "                fname = fname_acquisition_trucks(dirs_ts, bbox_id, date_str, ext_arr)\n",
    "                if os.path.exists(fname):\n",
    "                    period.files.append(fname)\n",
    "                else:\n",
    "                    weekday_match = any([is_weekday(period_date, w) for w in weekdays])\n",
    "                    if weekday_match:\n",
    "                        acquisition.mask_clouds()\n",
    "                        # check if enough observations to be considered\n",
    "                        if acquisition.has_observations(minimum_valid_observations):\n",
    "                            print(\"Processing acquisition: \" + date_str)\n",
    "                            period.files.append(fname)\n",
    "                            try:\n",
    "                                acquisition.mask_with_osm(period.osm_mask) # mask to OSM roads\n",
    "                                acquisition.do_detection(thresholds) # truck detection\n",
    "                                print(\"Writing\")\n",
    "                                acquisition.write_detections(fname)\n",
    "                            except:\n",
    "                                print(\"Failed: %s\" %(date_str))\n",
    "                            print(\"Done with acquisition: %s\" %(date_str))\n",
    "            if len(period.files) > 0:\n",
    "                # read all processed files from disk in order not to hold in memory during processing\n",
    "                print(\"Aggregating sub-period\")\n",
    "                for file in period.files:\n",
    "                    if os.path.exists(file):\n",
    "                        try:\n",
    "                            acquisition.read_detections(file)\n",
    "                            period.add_n_observations(acquisition.no_clouds)\n",
    "                            period.add_detections(acquisition.detector.trucks.values)\n",
    "                        except:\n",
    "                            Warning(\"Could not read: \" + fname)\n",
    "                period.wrap_period(fnames)\n",
    "            else:\n",
    "                msg = \"No acquisitions in period %s to %s. In bbox_id: %s\" %(str(start), str(end), bbox_id_str)\n",
    "                Warning(msg)\n",
    "                trace.append(msg)\n",
    "            print(\"Processing sub-period took: %s minutes\" %(str((datetime.now()-tracker_start).seconds/60)))\n",
    "        print(\"Done with period %s of bbox_id %s\\n%s\" %(ts_str, bbox_id_str, sep))    \n",
    "    print(\"Done with bbox_id: \" + bbox_id_str)\n",
    "print(\"%s\\nDone with all requested bboxes\\n\" %(sep, sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-period-wise read points layer of bounding boxes\n",
    "# intersect layers country-wise with country borders\n",
    "# write statistics country-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
